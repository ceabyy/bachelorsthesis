name: "MemNet_M10R10_212C64"
input: "data"
input_dim: 1
input_dim: 1
input_dim: 130
input_dim: 130
layer {
    name: "bn_conv1"
    type: "BatchNorm"
    bottom: "data"
    top: "bn_conv1"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv1"
    type: "Scale"
    bottom: "bn_conv1"
    top: "bn_conv1"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu1"
    type: "ReLU"
    bottom: "bn_conv1"
    top: "bn_conv1"
}
layer {
    name: "conv1"
    type: "Convolution"
    bottom: "bn_conv1"
    top: "conv1"
    param {
        lr_mult: 1.000000
    }
    param {
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv01_01_a"
    type: "BatchNorm"
    bottom: "conv1"
    top: "bn_conv01_01_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv01_01_a"
    type: "Scale"
    bottom: "bn_conv01_01_a"
    top: "bn_conv01_01_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu01_01_a"
    type: "ReLU"
    bottom: "bn_conv01_01_a"
    top: "bn_conv01_01_a"
}
layer {
    name: "conv01_01_a"
    type: "Convolution"
    bottom: "bn_conv01_01_a"
    top: "conv01_01_a"
    param {
        name: "conv01_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv01_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv01_01_b"
    type: "BatchNorm"
    bottom: "conv01_01_a"
    top: "bn_conv01_01_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv01_01_b"
    type: "Scale"
    bottom: "bn_conv01_01_b"
    top: "bn_conv01_01_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu01_01_b"
    type: "ReLU"
    bottom: "bn_conv01_01_b"
    top: "bn_conv01_01_b"
}
layer {
    name: "conv01_01_b"
    type: "Convolution"
    bottom: "bn_conv01_01_b"
    top: "conv01_01_b"
    param {
        name: "conv01_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv01_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise01_01"
    type: "Eltwise"
    bottom: "conv1"
    bottom: "conv01_01_b"
    top: "eltwise01_01"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv01_02_a"
    type: "BatchNorm"
    bottom: "eltwise01_01"
    top: "bn_conv01_02_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv01_02_a"
    type: "Scale"
    bottom: "bn_conv01_02_a"
    top: "bn_conv01_02_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu01_02_a"
    type: "ReLU"
    bottom: "bn_conv01_02_a"
    top: "bn_conv01_02_a"
}
layer {
    name: "conv01_02_a"
    type: "Convolution"
    bottom: "bn_conv01_02_a"
    top: "conv01_02_a"
    param {
        name: "conv01_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv01_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv01_02_b"
    type: "BatchNorm"
    bottom: "conv01_02_a"
    top: "bn_conv01_02_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv01_02_b"
    type: "Scale"
    bottom: "bn_conv01_02_b"
    top: "bn_conv01_02_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu01_02_b"
    type: "ReLU"
    bottom: "bn_conv01_02_b"
    top: "bn_conv01_02_b"
}
layer {
    name: "conv01_02_b"
    type: "Convolution"
    bottom: "bn_conv01_02_b"
    top: "conv01_02_b"
    param {
        name: "conv01_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv01_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise01_02"
    type: "Eltwise"
    bottom: "eltwise01_01"
    bottom: "conv01_02_b"
    top: "eltwise01_02"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv01_03_a"
    type: "BatchNorm"
    bottom: "eltwise01_02"
    top: "bn_conv01_03_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv01_03_a"
    type: "Scale"
    bottom: "bn_conv01_03_a"
    top: "bn_conv01_03_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu01_03_a"
    type: "ReLU"
    bottom: "bn_conv01_03_a"
    top: "bn_conv01_03_a"
}
layer {
    name: "conv01_03_a"
    type: "Convolution"
    bottom: "bn_conv01_03_a"
    top: "conv01_03_a"
    param {
        name: "conv01_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv01_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv01_03_b"
    type: "BatchNorm"
    bottom: "conv01_03_a"
    top: "bn_conv01_03_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv01_03_b"
    type: "Scale"
    bottom: "bn_conv01_03_b"
    top: "bn_conv01_03_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu01_03_b"
    type: "ReLU"
    bottom: "bn_conv01_03_b"
    top: "bn_conv01_03_b"
}
layer {
    name: "conv01_03_b"
    type: "Convolution"
    bottom: "bn_conv01_03_b"
    top: "conv01_03_b"
    param {
        name: "conv01_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv01_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise01_03"
    type: "Eltwise"
    bottom: "eltwise01_02"
    bottom: "conv01_03_b"
    top: "eltwise01_03"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv01_04_a"
    type: "BatchNorm"
    bottom: "eltwise01_03"
    top: "bn_conv01_04_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv01_04_a"
    type: "Scale"
    bottom: "bn_conv01_04_a"
    top: "bn_conv01_04_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu01_04_a"
    type: "ReLU"
    bottom: "bn_conv01_04_a"
    top: "bn_conv01_04_a"
}
layer {
    name: "conv01_04_a"
    type: "Convolution"
    bottom: "bn_conv01_04_a"
    top: "conv01_04_a"
    param {
        name: "conv01_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv01_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv01_04_b"
    type: "BatchNorm"
    bottom: "conv01_04_a"
    top: "bn_conv01_04_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv01_04_b"
    type: "Scale"
    bottom: "bn_conv01_04_b"
    top: "bn_conv01_04_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu01_04_b"
    type: "ReLU"
    bottom: "bn_conv01_04_b"
    top: "bn_conv01_04_b"
}
layer {
    name: "conv01_04_b"
    type: "Convolution"
    bottom: "bn_conv01_04_b"
    top: "conv01_04_b"
    param {
        name: "conv01_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv01_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise01_04"
    type: "Eltwise"
    bottom: "eltwise01_03"
    bottom: "conv01_04_b"
    top: "eltwise01_04"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv01_05_a"
    type: "BatchNorm"
    bottom: "eltwise01_04"
    top: "bn_conv01_05_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv01_05_a"
    type: "Scale"
    bottom: "bn_conv01_05_a"
    top: "bn_conv01_05_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu01_05_a"
    type: "ReLU"
    bottom: "bn_conv01_05_a"
    top: "bn_conv01_05_a"
}
layer {
    name: "conv01_05_a"
    type: "Convolution"
    bottom: "bn_conv01_05_a"
    top: "conv01_05_a"
    param {
        name: "conv01_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv01_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv01_05_b"
    type: "BatchNorm"
    bottom: "conv01_05_a"
    top: "bn_conv01_05_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv01_05_b"
    type: "Scale"
    bottom: "bn_conv01_05_b"
    top: "bn_conv01_05_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu01_05_b"
    type: "ReLU"
    bottom: "bn_conv01_05_b"
    top: "bn_conv01_05_b"
}
layer {
    name: "conv01_05_b"
    type: "Convolution"
    bottom: "bn_conv01_05_b"
    top: "conv01_05_b"
    param {
        name: "conv01_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv01_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise01_05"
    type: "Eltwise"
    bottom: "eltwise01_04"
    bottom: "conv01_05_b"
    top: "eltwise01_05"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv01_06_a"
    type: "BatchNorm"
    bottom: "eltwise01_05"
    top: "bn_conv01_06_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv01_06_a"
    type: "Scale"
    bottom: "bn_conv01_06_a"
    top: "bn_conv01_06_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu01_06_a"
    type: "ReLU"
    bottom: "bn_conv01_06_a"
    top: "bn_conv01_06_a"
}
layer {
    name: "conv01_06_a"
    type: "Convolution"
    bottom: "bn_conv01_06_a"
    top: "conv01_06_a"
    param {
        name: "conv01_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv01_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv01_06_b"
    type: "BatchNorm"
    bottom: "conv01_06_a"
    top: "bn_conv01_06_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv01_06_b"
    type: "Scale"
    bottom: "bn_conv01_06_b"
    top: "bn_conv01_06_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu01_06_b"
    type: "ReLU"
    bottom: "bn_conv01_06_b"
    top: "bn_conv01_06_b"
}
layer {
    name: "conv01_06_b"
    type: "Convolution"
    bottom: "bn_conv01_06_b"
    top: "conv01_06_b"
    param {
        name: "conv01_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv01_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise01_06"
    type: "Eltwise"
    bottom: "eltwise01_05"
    bottom: "conv01_06_b"
    top: "eltwise01_06"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv01_07_a"
    type: "BatchNorm"
    bottom: "eltwise01_06"
    top: "bn_conv01_07_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv01_07_a"
    type: "Scale"
    bottom: "bn_conv01_07_a"
    top: "bn_conv01_07_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu01_07_a"
    type: "ReLU"
    bottom: "bn_conv01_07_a"
    top: "bn_conv01_07_a"
}
layer {
    name: "conv01_07_a"
    type: "Convolution"
    bottom: "bn_conv01_07_a"
    top: "conv01_07_a"
    param {
        name: "conv01_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv01_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv01_07_b"
    type: "BatchNorm"
    bottom: "conv01_07_a"
    top: "bn_conv01_07_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv01_07_b"
    type: "Scale"
    bottom: "bn_conv01_07_b"
    top: "bn_conv01_07_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu01_07_b"
    type: "ReLU"
    bottom: "bn_conv01_07_b"
    top: "bn_conv01_07_b"
}
layer {
    name: "conv01_07_b"
    type: "Convolution"
    bottom: "bn_conv01_07_b"
    top: "conv01_07_b"
    param {
        name: "conv01_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv01_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise01_07"
    type: "Eltwise"
    bottom: "eltwise01_06"
    bottom: "conv01_07_b"
    top: "eltwise01_07"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv01_08_a"
    type: "BatchNorm"
    bottom: "eltwise01_07"
    top: "bn_conv01_08_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv01_08_a"
    type: "Scale"
    bottom: "bn_conv01_08_a"
    top: "bn_conv01_08_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu01_08_a"
    type: "ReLU"
    bottom: "bn_conv01_08_a"
    top: "bn_conv01_08_a"
}
layer {
    name: "conv01_08_a"
    type: "Convolution"
    bottom: "bn_conv01_08_a"
    top: "conv01_08_a"
    param {
        name: "conv01_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv01_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv01_08_b"
    type: "BatchNorm"
    bottom: "conv01_08_a"
    top: "bn_conv01_08_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv01_08_b"
    type: "Scale"
    bottom: "bn_conv01_08_b"
    top: "bn_conv01_08_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu01_08_b"
    type: "ReLU"
    bottom: "bn_conv01_08_b"
    top: "bn_conv01_08_b"
}
layer {
    name: "conv01_08_b"
    type: "Convolution"
    bottom: "bn_conv01_08_b"
    top: "conv01_08_b"
    param {
        name: "conv01_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv01_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise01_08"
    type: "Eltwise"
    bottom: "eltwise01_07"
    bottom: "conv01_08_b"
    top: "eltwise01_08"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv01_09_a"
    type: "BatchNorm"
    bottom: "eltwise01_08"
    top: "bn_conv01_09_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv01_09_a"
    type: "Scale"
    bottom: "bn_conv01_09_a"
    top: "bn_conv01_09_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu01_09_a"
    type: "ReLU"
    bottom: "bn_conv01_09_a"
    top: "bn_conv01_09_a"
}
layer {
    name: "conv01_09_a"
    type: "Convolution"
    bottom: "bn_conv01_09_a"
    top: "conv01_09_a"
    param {
        name: "conv01_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv01_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv01_09_b"
    type: "BatchNorm"
    bottom: "conv01_09_a"
    top: "bn_conv01_09_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv01_09_b"
    type: "Scale"
    bottom: "bn_conv01_09_b"
    top: "bn_conv01_09_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu01_09_b"
    type: "ReLU"
    bottom: "bn_conv01_09_b"
    top: "bn_conv01_09_b"
}
layer {
    name: "conv01_09_b"
    type: "Convolution"
    bottom: "bn_conv01_09_b"
    top: "conv01_09_b"
    param {
        name: "conv01_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv01_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise01_09"
    type: "Eltwise"
    bottom: "eltwise01_08"
    bottom: "conv01_09_b"
    top: "eltwise01_09"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv01_10_a"
    type: "BatchNorm"
    bottom: "eltwise01_09"
    top: "bn_conv01_10_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv01_10_a"
    type: "Scale"
    bottom: "bn_conv01_10_a"
    top: "bn_conv01_10_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu01_10_a"
    type: "ReLU"
    bottom: "bn_conv01_10_a"
    top: "bn_conv01_10_a"
}
layer {
    name: "conv01_10_a"
    type: "Convolution"
    bottom: "bn_conv01_10_a"
    top: "conv01_10_a"
    param {
        name: "conv01_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv01_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv01_10_b"
    type: "BatchNorm"
    bottom: "conv01_10_a"
    top: "bn_conv01_10_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv01_10_b"
    type: "Scale"
    bottom: "bn_conv01_10_b"
    top: "bn_conv01_10_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu01_10_b"
    type: "ReLU"
    bottom: "bn_conv01_10_b"
    top: "bn_conv01_10_b"
}
layer {
    name: "conv01_10_b"
    type: "Convolution"
    bottom: "bn_conv01_10_b"
    top: "conv01_10_b"
    param {
        name: "conv01_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv01_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise01_10"
    type: "Eltwise"
    bottom: "eltwise01_09"
    bottom: "conv01_10_b"
    top: "eltwise01_10"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "concat01"
    type: "Concat"
    bottom: "conv1"
    bottom: "eltwise01_01"
    bottom: "eltwise01_02"
    bottom: "eltwise01_03"
    bottom: "eltwise01_04"
    bottom: "eltwise01_05"
    bottom: "eltwise01_06"
    bottom: "eltwise01_07"
    bottom: "eltwise01_08"
    bottom: "eltwise01_09"
    bottom: "eltwise01_10"
    top: "concat01"
    concat_param {
        axis: 1
    }
}
layer {
    name: "bn_conv_transition_01"
    type: "BatchNorm"
    bottom: "concat01"
    top: "bn_conv_transition_01"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv_transition_01"
    type: "Scale"
    bottom: "bn_conv_transition_01"
    top: "bn_conv_transition_01"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_transition_01"
    type: "ReLU"
    bottom: "bn_conv_transition_01"
    top: "bn_conv_transition_01"
}
layer {
    name: "conv_transition_01"
    type: "Convolution"
    bottom: "bn_conv_transition_01"
    top: "conv_transition_01"
    param {
        lr_mult: 1.000000
    }
    param {
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 1
        stride: 1
        pad: 0
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv02_01_a"
    type: "BatchNorm"
    bottom: "conv_transition_01"
    top: "bn_conv02_01_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv02_01_a"
    type: "Scale"
    bottom: "bn_conv02_01_a"
    top: "bn_conv02_01_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu02_01_a"
    type: "ReLU"
    bottom: "bn_conv02_01_a"
    top: "bn_conv02_01_a"
}
layer {
    name: "conv02_01_a"
    type: "Convolution"
    bottom: "bn_conv02_01_a"
    top: "conv02_01_a"
    param {
        name: "conv02_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv02_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv02_01_b"
    type: "BatchNorm"
    bottom: "conv02_01_a"
    top: "bn_conv02_01_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv02_01_b"
    type: "Scale"
    bottom: "bn_conv02_01_b"
    top: "bn_conv02_01_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu02_01_b"
    type: "ReLU"
    bottom: "bn_conv02_01_b"
    top: "bn_conv02_01_b"
}
layer {
    name: "conv02_01_b"
    type: "Convolution"
    bottom: "bn_conv02_01_b"
    top: "conv02_01_b"
    param {
        name: "conv02_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv02_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise02_01"
    type: "Eltwise"
    bottom: "conv_transition_01"
    bottom: "conv02_01_b"
    top: "eltwise02_01"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv02_02_a"
    type: "BatchNorm"
    bottom: "eltwise02_01"
    top: "bn_conv02_02_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv02_02_a"
    type: "Scale"
    bottom: "bn_conv02_02_a"
    top: "bn_conv02_02_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu02_02_a"
    type: "ReLU"
    bottom: "bn_conv02_02_a"
    top: "bn_conv02_02_a"
}
layer {
    name: "conv02_02_a"
    type: "Convolution"
    bottom: "bn_conv02_02_a"
    top: "conv02_02_a"
    param {
        name: "conv02_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv02_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv02_02_b"
    type: "BatchNorm"
    bottom: "conv02_02_a"
    top: "bn_conv02_02_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv02_02_b"
    type: "Scale"
    bottom: "bn_conv02_02_b"
    top: "bn_conv02_02_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu02_02_b"
    type: "ReLU"
    bottom: "bn_conv02_02_b"
    top: "bn_conv02_02_b"
}
layer {
    name: "conv02_02_b"
    type: "Convolution"
    bottom: "bn_conv02_02_b"
    top: "conv02_02_b"
    param {
        name: "conv02_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv02_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise02_02"
    type: "Eltwise"
    bottom: "eltwise02_01"
    bottom: "conv02_02_b"
    top: "eltwise02_02"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv02_03_a"
    type: "BatchNorm"
    bottom: "eltwise02_02"
    top: "bn_conv02_03_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv02_03_a"
    type: "Scale"
    bottom: "bn_conv02_03_a"
    top: "bn_conv02_03_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu02_03_a"
    type: "ReLU"
    bottom: "bn_conv02_03_a"
    top: "bn_conv02_03_a"
}
layer {
    name: "conv02_03_a"
    type: "Convolution"
    bottom: "bn_conv02_03_a"
    top: "conv02_03_a"
    param {
        name: "conv02_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv02_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv02_03_b"
    type: "BatchNorm"
    bottom: "conv02_03_a"
    top: "bn_conv02_03_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv02_03_b"
    type: "Scale"
    bottom: "bn_conv02_03_b"
    top: "bn_conv02_03_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu02_03_b"
    type: "ReLU"
    bottom: "bn_conv02_03_b"
    top: "bn_conv02_03_b"
}
layer {
    name: "conv02_03_b"
    type: "Convolution"
    bottom: "bn_conv02_03_b"
    top: "conv02_03_b"
    param {
        name: "conv02_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv02_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise02_03"
    type: "Eltwise"
    bottom: "eltwise02_02"
    bottom: "conv02_03_b"
    top: "eltwise02_03"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv02_04_a"
    type: "BatchNorm"
    bottom: "eltwise02_03"
    top: "bn_conv02_04_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv02_04_a"
    type: "Scale"
    bottom: "bn_conv02_04_a"
    top: "bn_conv02_04_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu02_04_a"
    type: "ReLU"
    bottom: "bn_conv02_04_a"
    top: "bn_conv02_04_a"
}
layer {
    name: "conv02_04_a"
    type: "Convolution"
    bottom: "bn_conv02_04_a"
    top: "conv02_04_a"
    param {
        name: "conv02_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv02_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv02_04_b"
    type: "BatchNorm"
    bottom: "conv02_04_a"
    top: "bn_conv02_04_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv02_04_b"
    type: "Scale"
    bottom: "bn_conv02_04_b"
    top: "bn_conv02_04_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu02_04_b"
    type: "ReLU"
    bottom: "bn_conv02_04_b"
    top: "bn_conv02_04_b"
}
layer {
    name: "conv02_04_b"
    type: "Convolution"
    bottom: "bn_conv02_04_b"
    top: "conv02_04_b"
    param {
        name: "conv02_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv02_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise02_04"
    type: "Eltwise"
    bottom: "eltwise02_03"
    bottom: "conv02_04_b"
    top: "eltwise02_04"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv02_05_a"
    type: "BatchNorm"
    bottom: "eltwise02_04"
    top: "bn_conv02_05_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv02_05_a"
    type: "Scale"
    bottom: "bn_conv02_05_a"
    top: "bn_conv02_05_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu02_05_a"
    type: "ReLU"
    bottom: "bn_conv02_05_a"
    top: "bn_conv02_05_a"
}
layer {
    name: "conv02_05_a"
    type: "Convolution"
    bottom: "bn_conv02_05_a"
    top: "conv02_05_a"
    param {
        name: "conv02_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv02_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv02_05_b"
    type: "BatchNorm"
    bottom: "conv02_05_a"
    top: "bn_conv02_05_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv02_05_b"
    type: "Scale"
    bottom: "bn_conv02_05_b"
    top: "bn_conv02_05_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu02_05_b"
    type: "ReLU"
    bottom: "bn_conv02_05_b"
    top: "bn_conv02_05_b"
}
layer {
    name: "conv02_05_b"
    type: "Convolution"
    bottom: "bn_conv02_05_b"
    top: "conv02_05_b"
    param {
        name: "conv02_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv02_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise02_05"
    type: "Eltwise"
    bottom: "eltwise02_04"
    bottom: "conv02_05_b"
    top: "eltwise02_05"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv02_06_a"
    type: "BatchNorm"
    bottom: "eltwise02_05"
    top: "bn_conv02_06_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv02_06_a"
    type: "Scale"
    bottom: "bn_conv02_06_a"
    top: "bn_conv02_06_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu02_06_a"
    type: "ReLU"
    bottom: "bn_conv02_06_a"
    top: "bn_conv02_06_a"
}
layer {
    name: "conv02_06_a"
    type: "Convolution"
    bottom: "bn_conv02_06_a"
    top: "conv02_06_a"
    param {
        name: "conv02_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv02_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv02_06_b"
    type: "BatchNorm"
    bottom: "conv02_06_a"
    top: "bn_conv02_06_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv02_06_b"
    type: "Scale"
    bottom: "bn_conv02_06_b"
    top: "bn_conv02_06_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu02_06_b"
    type: "ReLU"
    bottom: "bn_conv02_06_b"
    top: "bn_conv02_06_b"
}
layer {
    name: "conv02_06_b"
    type: "Convolution"
    bottom: "bn_conv02_06_b"
    top: "conv02_06_b"
    param {
        name: "conv02_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv02_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise02_06"
    type: "Eltwise"
    bottom: "eltwise02_05"
    bottom: "conv02_06_b"
    top: "eltwise02_06"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv02_07_a"
    type: "BatchNorm"
    bottom: "eltwise02_06"
    top: "bn_conv02_07_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv02_07_a"
    type: "Scale"
    bottom: "bn_conv02_07_a"
    top: "bn_conv02_07_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu02_07_a"
    type: "ReLU"
    bottom: "bn_conv02_07_a"
    top: "bn_conv02_07_a"
}
layer {
    name: "conv02_07_a"
    type: "Convolution"
    bottom: "bn_conv02_07_a"
    top: "conv02_07_a"
    param {
        name: "conv02_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv02_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv02_07_b"
    type: "BatchNorm"
    bottom: "conv02_07_a"
    top: "bn_conv02_07_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv02_07_b"
    type: "Scale"
    bottom: "bn_conv02_07_b"
    top: "bn_conv02_07_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu02_07_b"
    type: "ReLU"
    bottom: "bn_conv02_07_b"
    top: "bn_conv02_07_b"
}
layer {
    name: "conv02_07_b"
    type: "Convolution"
    bottom: "bn_conv02_07_b"
    top: "conv02_07_b"
    param {
        name: "conv02_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv02_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise02_07"
    type: "Eltwise"
    bottom: "eltwise02_06"
    bottom: "conv02_07_b"
    top: "eltwise02_07"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv02_08_a"
    type: "BatchNorm"
    bottom: "eltwise02_07"
    top: "bn_conv02_08_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv02_08_a"
    type: "Scale"
    bottom: "bn_conv02_08_a"
    top: "bn_conv02_08_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu02_08_a"
    type: "ReLU"
    bottom: "bn_conv02_08_a"
    top: "bn_conv02_08_a"
}
layer {
    name: "conv02_08_a"
    type: "Convolution"
    bottom: "bn_conv02_08_a"
    top: "conv02_08_a"
    param {
        name: "conv02_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv02_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv02_08_b"
    type: "BatchNorm"
    bottom: "conv02_08_a"
    top: "bn_conv02_08_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv02_08_b"
    type: "Scale"
    bottom: "bn_conv02_08_b"
    top: "bn_conv02_08_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu02_08_b"
    type: "ReLU"
    bottom: "bn_conv02_08_b"
    top: "bn_conv02_08_b"
}
layer {
    name: "conv02_08_b"
    type: "Convolution"
    bottom: "bn_conv02_08_b"
    top: "conv02_08_b"
    param {
        name: "conv02_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv02_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise02_08"
    type: "Eltwise"
    bottom: "eltwise02_07"
    bottom: "conv02_08_b"
    top: "eltwise02_08"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv02_09_a"
    type: "BatchNorm"
    bottom: "eltwise02_08"
    top: "bn_conv02_09_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv02_09_a"
    type: "Scale"
    bottom: "bn_conv02_09_a"
    top: "bn_conv02_09_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu02_09_a"
    type: "ReLU"
    bottom: "bn_conv02_09_a"
    top: "bn_conv02_09_a"
}
layer {
    name: "conv02_09_a"
    type: "Convolution"
    bottom: "bn_conv02_09_a"
    top: "conv02_09_a"
    param {
        name: "conv02_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv02_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv02_09_b"
    type: "BatchNorm"
    bottom: "conv02_09_a"
    top: "bn_conv02_09_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv02_09_b"
    type: "Scale"
    bottom: "bn_conv02_09_b"
    top: "bn_conv02_09_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu02_09_b"
    type: "ReLU"
    bottom: "bn_conv02_09_b"
    top: "bn_conv02_09_b"
}
layer {
    name: "conv02_09_b"
    type: "Convolution"
    bottom: "bn_conv02_09_b"
    top: "conv02_09_b"
    param {
        name: "conv02_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv02_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise02_09"
    type: "Eltwise"
    bottom: "eltwise02_08"
    bottom: "conv02_09_b"
    top: "eltwise02_09"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv02_10_a"
    type: "BatchNorm"
    bottom: "eltwise02_09"
    top: "bn_conv02_10_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv02_10_a"
    type: "Scale"
    bottom: "bn_conv02_10_a"
    top: "bn_conv02_10_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu02_10_a"
    type: "ReLU"
    bottom: "bn_conv02_10_a"
    top: "bn_conv02_10_a"
}
layer {
    name: "conv02_10_a"
    type: "Convolution"
    bottom: "bn_conv02_10_a"
    top: "conv02_10_a"
    param {
        name: "conv02_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv02_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv02_10_b"
    type: "BatchNorm"
    bottom: "conv02_10_a"
    top: "bn_conv02_10_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv02_10_b"
    type: "Scale"
    bottom: "bn_conv02_10_b"
    top: "bn_conv02_10_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu02_10_b"
    type: "ReLU"
    bottom: "bn_conv02_10_b"
    top: "bn_conv02_10_b"
}
layer {
    name: "conv02_10_b"
    type: "Convolution"
    bottom: "bn_conv02_10_b"
    top: "conv02_10_b"
    param {
        name: "conv02_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv02_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise02_10"
    type: "Eltwise"
    bottom: "eltwise02_09"
    bottom: "conv02_10_b"
    top: "eltwise02_10"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "concat02"
    type: "Concat"
    bottom: "conv1"
    bottom: "conv_transition_01"
    bottom: "eltwise02_01"
    bottom: "eltwise02_02"
    bottom: "eltwise02_03"
    bottom: "eltwise02_04"
    bottom: "eltwise02_05"
    bottom: "eltwise02_06"
    bottom: "eltwise02_07"
    bottom: "eltwise02_08"
    bottom: "eltwise02_09"
    bottom: "eltwise02_10"
    top: "concat02"
    concat_param {
        axis: 1
    }
}
layer {
    name: "bn_conv_transition_02"
    type: "BatchNorm"
    bottom: "concat02"
    top: "bn_conv_transition_02"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv_transition_02"
    type: "Scale"
    bottom: "bn_conv_transition_02"
    top: "bn_conv_transition_02"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_transition_02"
    type: "ReLU"
    bottom: "bn_conv_transition_02"
    top: "bn_conv_transition_02"
}
layer {
    name: "conv_transition_02"
    type: "Convolution"
    bottom: "bn_conv_transition_02"
    top: "conv_transition_02"
    param {
        lr_mult: 1.000000
    }
    param {
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 1
        stride: 1
        pad: 0
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv03_01_a"
    type: "BatchNorm"
    bottom: "conv_transition_02"
    top: "bn_conv03_01_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv03_01_a"
    type: "Scale"
    bottom: "bn_conv03_01_a"
    top: "bn_conv03_01_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu03_01_a"
    type: "ReLU"
    bottom: "bn_conv03_01_a"
    top: "bn_conv03_01_a"
}
layer {
    name: "conv03_01_a"
    type: "Convolution"
    bottom: "bn_conv03_01_a"
    top: "conv03_01_a"
    param {
        name: "conv03_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv03_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv03_01_b"
    type: "BatchNorm"
    bottom: "conv03_01_a"
    top: "bn_conv03_01_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv03_01_b"
    type: "Scale"
    bottom: "bn_conv03_01_b"
    top: "bn_conv03_01_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu03_01_b"
    type: "ReLU"
    bottom: "bn_conv03_01_b"
    top: "bn_conv03_01_b"
}
layer {
    name: "conv03_01_b"
    type: "Convolution"
    bottom: "bn_conv03_01_b"
    top: "conv03_01_b"
    param {
        name: "conv03_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv03_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise03_01"
    type: "Eltwise"
    bottom: "conv_transition_02"
    bottom: "conv03_01_b"
    top: "eltwise03_01"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv03_02_a"
    type: "BatchNorm"
    bottom: "eltwise03_01"
    top: "bn_conv03_02_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv03_02_a"
    type: "Scale"
    bottom: "bn_conv03_02_a"
    top: "bn_conv03_02_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu03_02_a"
    type: "ReLU"
    bottom: "bn_conv03_02_a"
    top: "bn_conv03_02_a"
}
layer {
    name: "conv03_02_a"
    type: "Convolution"
    bottom: "bn_conv03_02_a"
    top: "conv03_02_a"
    param {
        name: "conv03_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv03_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv03_02_b"
    type: "BatchNorm"
    bottom: "conv03_02_a"
    top: "bn_conv03_02_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv03_02_b"
    type: "Scale"
    bottom: "bn_conv03_02_b"
    top: "bn_conv03_02_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu03_02_b"
    type: "ReLU"
    bottom: "bn_conv03_02_b"
    top: "bn_conv03_02_b"
}
layer {
    name: "conv03_02_b"
    type: "Convolution"
    bottom: "bn_conv03_02_b"
    top: "conv03_02_b"
    param {
        name: "conv03_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv03_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise03_02"
    type: "Eltwise"
    bottom: "eltwise03_01"
    bottom: "conv03_02_b"
    top: "eltwise03_02"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv03_03_a"
    type: "BatchNorm"
    bottom: "eltwise03_02"
    top: "bn_conv03_03_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv03_03_a"
    type: "Scale"
    bottom: "bn_conv03_03_a"
    top: "bn_conv03_03_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu03_03_a"
    type: "ReLU"
    bottom: "bn_conv03_03_a"
    top: "bn_conv03_03_a"
}
layer {
    name: "conv03_03_a"
    type: "Convolution"
    bottom: "bn_conv03_03_a"
    top: "conv03_03_a"
    param {
        name: "conv03_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv03_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv03_03_b"
    type: "BatchNorm"
    bottom: "conv03_03_a"
    top: "bn_conv03_03_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv03_03_b"
    type: "Scale"
    bottom: "bn_conv03_03_b"
    top: "bn_conv03_03_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu03_03_b"
    type: "ReLU"
    bottom: "bn_conv03_03_b"
    top: "bn_conv03_03_b"
}
layer {
    name: "conv03_03_b"
    type: "Convolution"
    bottom: "bn_conv03_03_b"
    top: "conv03_03_b"
    param {
        name: "conv03_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv03_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise03_03"
    type: "Eltwise"
    bottom: "eltwise03_02"
    bottom: "conv03_03_b"
    top: "eltwise03_03"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv03_04_a"
    type: "BatchNorm"
    bottom: "eltwise03_03"
    top: "bn_conv03_04_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv03_04_a"
    type: "Scale"
    bottom: "bn_conv03_04_a"
    top: "bn_conv03_04_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu03_04_a"
    type: "ReLU"
    bottom: "bn_conv03_04_a"
    top: "bn_conv03_04_a"
}
layer {
    name: "conv03_04_a"
    type: "Convolution"
    bottom: "bn_conv03_04_a"
    top: "conv03_04_a"
    param {
        name: "conv03_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv03_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv03_04_b"
    type: "BatchNorm"
    bottom: "conv03_04_a"
    top: "bn_conv03_04_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv03_04_b"
    type: "Scale"
    bottom: "bn_conv03_04_b"
    top: "bn_conv03_04_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu03_04_b"
    type: "ReLU"
    bottom: "bn_conv03_04_b"
    top: "bn_conv03_04_b"
}
layer {
    name: "conv03_04_b"
    type: "Convolution"
    bottom: "bn_conv03_04_b"
    top: "conv03_04_b"
    param {
        name: "conv03_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv03_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise03_04"
    type: "Eltwise"
    bottom: "eltwise03_03"
    bottom: "conv03_04_b"
    top: "eltwise03_04"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv03_05_a"
    type: "BatchNorm"
    bottom: "eltwise03_04"
    top: "bn_conv03_05_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv03_05_a"
    type: "Scale"
    bottom: "bn_conv03_05_a"
    top: "bn_conv03_05_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu03_05_a"
    type: "ReLU"
    bottom: "bn_conv03_05_a"
    top: "bn_conv03_05_a"
}
layer {
    name: "conv03_05_a"
    type: "Convolution"
    bottom: "bn_conv03_05_a"
    top: "conv03_05_a"
    param {
        name: "conv03_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv03_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv03_05_b"
    type: "BatchNorm"
    bottom: "conv03_05_a"
    top: "bn_conv03_05_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv03_05_b"
    type: "Scale"
    bottom: "bn_conv03_05_b"
    top: "bn_conv03_05_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu03_05_b"
    type: "ReLU"
    bottom: "bn_conv03_05_b"
    top: "bn_conv03_05_b"
}
layer {
    name: "conv03_05_b"
    type: "Convolution"
    bottom: "bn_conv03_05_b"
    top: "conv03_05_b"
    param {
        name: "conv03_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv03_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise03_05"
    type: "Eltwise"
    bottom: "eltwise03_04"
    bottom: "conv03_05_b"
    top: "eltwise03_05"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv03_06_a"
    type: "BatchNorm"
    bottom: "eltwise03_05"
    top: "bn_conv03_06_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv03_06_a"
    type: "Scale"
    bottom: "bn_conv03_06_a"
    top: "bn_conv03_06_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu03_06_a"
    type: "ReLU"
    bottom: "bn_conv03_06_a"
    top: "bn_conv03_06_a"
}
layer {
    name: "conv03_06_a"
    type: "Convolution"
    bottom: "bn_conv03_06_a"
    top: "conv03_06_a"
    param {
        name: "conv03_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv03_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv03_06_b"
    type: "BatchNorm"
    bottom: "conv03_06_a"
    top: "bn_conv03_06_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv03_06_b"
    type: "Scale"
    bottom: "bn_conv03_06_b"
    top: "bn_conv03_06_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu03_06_b"
    type: "ReLU"
    bottom: "bn_conv03_06_b"
    top: "bn_conv03_06_b"
}
layer {
    name: "conv03_06_b"
    type: "Convolution"
    bottom: "bn_conv03_06_b"
    top: "conv03_06_b"
    param {
        name: "conv03_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv03_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise03_06"
    type: "Eltwise"
    bottom: "eltwise03_05"
    bottom: "conv03_06_b"
    top: "eltwise03_06"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv03_07_a"
    type: "BatchNorm"
    bottom: "eltwise03_06"
    top: "bn_conv03_07_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv03_07_a"
    type: "Scale"
    bottom: "bn_conv03_07_a"
    top: "bn_conv03_07_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu03_07_a"
    type: "ReLU"
    bottom: "bn_conv03_07_a"
    top: "bn_conv03_07_a"
}
layer {
    name: "conv03_07_a"
    type: "Convolution"
    bottom: "bn_conv03_07_a"
    top: "conv03_07_a"
    param {
        name: "conv03_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv03_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv03_07_b"
    type: "BatchNorm"
    bottom: "conv03_07_a"
    top: "bn_conv03_07_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv03_07_b"
    type: "Scale"
    bottom: "bn_conv03_07_b"
    top: "bn_conv03_07_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu03_07_b"
    type: "ReLU"
    bottom: "bn_conv03_07_b"
    top: "bn_conv03_07_b"
}
layer {
    name: "conv03_07_b"
    type: "Convolution"
    bottom: "bn_conv03_07_b"
    top: "conv03_07_b"
    param {
        name: "conv03_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv03_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise03_07"
    type: "Eltwise"
    bottom: "eltwise03_06"
    bottom: "conv03_07_b"
    top: "eltwise03_07"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv03_08_a"
    type: "BatchNorm"
    bottom: "eltwise03_07"
    top: "bn_conv03_08_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv03_08_a"
    type: "Scale"
    bottom: "bn_conv03_08_a"
    top: "bn_conv03_08_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu03_08_a"
    type: "ReLU"
    bottom: "bn_conv03_08_a"
    top: "bn_conv03_08_a"
}
layer {
    name: "conv03_08_a"
    type: "Convolution"
    bottom: "bn_conv03_08_a"
    top: "conv03_08_a"
    param {
        name: "conv03_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv03_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv03_08_b"
    type: "BatchNorm"
    bottom: "conv03_08_a"
    top: "bn_conv03_08_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv03_08_b"
    type: "Scale"
    bottom: "bn_conv03_08_b"
    top: "bn_conv03_08_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu03_08_b"
    type: "ReLU"
    bottom: "bn_conv03_08_b"
    top: "bn_conv03_08_b"
}
layer {
    name: "conv03_08_b"
    type: "Convolution"
    bottom: "bn_conv03_08_b"
    top: "conv03_08_b"
    param {
        name: "conv03_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv03_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise03_08"
    type: "Eltwise"
    bottom: "eltwise03_07"
    bottom: "conv03_08_b"
    top: "eltwise03_08"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv03_09_a"
    type: "BatchNorm"
    bottom: "eltwise03_08"
    top: "bn_conv03_09_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv03_09_a"
    type: "Scale"
    bottom: "bn_conv03_09_a"
    top: "bn_conv03_09_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu03_09_a"
    type: "ReLU"
    bottom: "bn_conv03_09_a"
    top: "bn_conv03_09_a"
}
layer {
    name: "conv03_09_a"
    type: "Convolution"
    bottom: "bn_conv03_09_a"
    top: "conv03_09_a"
    param {
        name: "conv03_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv03_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv03_09_b"
    type: "BatchNorm"
    bottom: "conv03_09_a"
    top: "bn_conv03_09_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv03_09_b"
    type: "Scale"
    bottom: "bn_conv03_09_b"
    top: "bn_conv03_09_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu03_09_b"
    type: "ReLU"
    bottom: "bn_conv03_09_b"
    top: "bn_conv03_09_b"
}
layer {
    name: "conv03_09_b"
    type: "Convolution"
    bottom: "bn_conv03_09_b"
    top: "conv03_09_b"
    param {
        name: "conv03_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv03_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise03_09"
    type: "Eltwise"
    bottom: "eltwise03_08"
    bottom: "conv03_09_b"
    top: "eltwise03_09"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv03_10_a"
    type: "BatchNorm"
    bottom: "eltwise03_09"
    top: "bn_conv03_10_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv03_10_a"
    type: "Scale"
    bottom: "bn_conv03_10_a"
    top: "bn_conv03_10_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu03_10_a"
    type: "ReLU"
    bottom: "bn_conv03_10_a"
    top: "bn_conv03_10_a"
}
layer {
    name: "conv03_10_a"
    type: "Convolution"
    bottom: "bn_conv03_10_a"
    top: "conv03_10_a"
    param {
        name: "conv03_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv03_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv03_10_b"
    type: "BatchNorm"
    bottom: "conv03_10_a"
    top: "bn_conv03_10_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv03_10_b"
    type: "Scale"
    bottom: "bn_conv03_10_b"
    top: "bn_conv03_10_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu03_10_b"
    type: "ReLU"
    bottom: "bn_conv03_10_b"
    top: "bn_conv03_10_b"
}
layer {
    name: "conv03_10_b"
    type: "Convolution"
    bottom: "bn_conv03_10_b"
    top: "conv03_10_b"
    param {
        name: "conv03_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv03_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise03_10"
    type: "Eltwise"
    bottom: "eltwise03_09"
    bottom: "conv03_10_b"
    top: "eltwise03_10"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "concat03"
    type: "Concat"
    bottom: "conv1"
    bottom: "conv_transition_01"
    bottom: "conv_transition_02"
    bottom: "eltwise03_01"
    bottom: "eltwise03_02"
    bottom: "eltwise03_03"
    bottom: "eltwise03_04"
    bottom: "eltwise03_05"
    bottom: "eltwise03_06"
    bottom: "eltwise03_07"
    bottom: "eltwise03_08"
    bottom: "eltwise03_09"
    bottom: "eltwise03_10"
    top: "concat03"
    concat_param {
        axis: 1
    }
}
layer {
    name: "bn_conv_transition_03"
    type: "BatchNorm"
    bottom: "concat03"
    top: "bn_conv_transition_03"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv_transition_03"
    type: "Scale"
    bottom: "bn_conv_transition_03"
    top: "bn_conv_transition_03"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_transition_03"
    type: "ReLU"
    bottom: "bn_conv_transition_03"
    top: "bn_conv_transition_03"
}
layer {
    name: "conv_transition_03"
    type: "Convolution"
    bottom: "bn_conv_transition_03"
    top: "conv_transition_03"
    param {
        lr_mult: 1.000000
    }
    param {
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 1
        stride: 1
        pad: 0
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv04_01_a"
    type: "BatchNorm"
    bottom: "conv_transition_03"
    top: "bn_conv04_01_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv04_01_a"
    type: "Scale"
    bottom: "bn_conv04_01_a"
    top: "bn_conv04_01_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu04_01_a"
    type: "ReLU"
    bottom: "bn_conv04_01_a"
    top: "bn_conv04_01_a"
}
layer {
    name: "conv04_01_a"
    type: "Convolution"
    bottom: "bn_conv04_01_a"
    top: "conv04_01_a"
    param {
        name: "conv04_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv04_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv04_01_b"
    type: "BatchNorm"
    bottom: "conv04_01_a"
    top: "bn_conv04_01_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv04_01_b"
    type: "Scale"
    bottom: "bn_conv04_01_b"
    top: "bn_conv04_01_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu04_01_b"
    type: "ReLU"
    bottom: "bn_conv04_01_b"
    top: "bn_conv04_01_b"
}
layer {
    name: "conv04_01_b"
    type: "Convolution"
    bottom: "bn_conv04_01_b"
    top: "conv04_01_b"
    param {
        name: "conv04_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv04_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise04_01"
    type: "Eltwise"
    bottom: "conv_transition_03"
    bottom: "conv04_01_b"
    top: "eltwise04_01"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv04_02_a"
    type: "BatchNorm"
    bottom: "eltwise04_01"
    top: "bn_conv04_02_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv04_02_a"
    type: "Scale"
    bottom: "bn_conv04_02_a"
    top: "bn_conv04_02_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu04_02_a"
    type: "ReLU"
    bottom: "bn_conv04_02_a"
    top: "bn_conv04_02_a"
}
layer {
    name: "conv04_02_a"
    type: "Convolution"
    bottom: "bn_conv04_02_a"
    top: "conv04_02_a"
    param {
        name: "conv04_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv04_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv04_02_b"
    type: "BatchNorm"
    bottom: "conv04_02_a"
    top: "bn_conv04_02_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv04_02_b"
    type: "Scale"
    bottom: "bn_conv04_02_b"
    top: "bn_conv04_02_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu04_02_b"
    type: "ReLU"
    bottom: "bn_conv04_02_b"
    top: "bn_conv04_02_b"
}
layer {
    name: "conv04_02_b"
    type: "Convolution"
    bottom: "bn_conv04_02_b"
    top: "conv04_02_b"
    param {
        name: "conv04_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv04_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise04_02"
    type: "Eltwise"
    bottom: "eltwise04_01"
    bottom: "conv04_02_b"
    top: "eltwise04_02"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv04_03_a"
    type: "BatchNorm"
    bottom: "eltwise04_02"
    top: "bn_conv04_03_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv04_03_a"
    type: "Scale"
    bottom: "bn_conv04_03_a"
    top: "bn_conv04_03_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu04_03_a"
    type: "ReLU"
    bottom: "bn_conv04_03_a"
    top: "bn_conv04_03_a"
}
layer {
    name: "conv04_03_a"
    type: "Convolution"
    bottom: "bn_conv04_03_a"
    top: "conv04_03_a"
    param {
        name: "conv04_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv04_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv04_03_b"
    type: "BatchNorm"
    bottom: "conv04_03_a"
    top: "bn_conv04_03_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv04_03_b"
    type: "Scale"
    bottom: "bn_conv04_03_b"
    top: "bn_conv04_03_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu04_03_b"
    type: "ReLU"
    bottom: "bn_conv04_03_b"
    top: "bn_conv04_03_b"
}
layer {
    name: "conv04_03_b"
    type: "Convolution"
    bottom: "bn_conv04_03_b"
    top: "conv04_03_b"
    param {
        name: "conv04_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv04_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise04_03"
    type: "Eltwise"
    bottom: "eltwise04_02"
    bottom: "conv04_03_b"
    top: "eltwise04_03"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv04_04_a"
    type: "BatchNorm"
    bottom: "eltwise04_03"
    top: "bn_conv04_04_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv04_04_a"
    type: "Scale"
    bottom: "bn_conv04_04_a"
    top: "bn_conv04_04_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu04_04_a"
    type: "ReLU"
    bottom: "bn_conv04_04_a"
    top: "bn_conv04_04_a"
}
layer {
    name: "conv04_04_a"
    type: "Convolution"
    bottom: "bn_conv04_04_a"
    top: "conv04_04_a"
    param {
        name: "conv04_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv04_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv04_04_b"
    type: "BatchNorm"
    bottom: "conv04_04_a"
    top: "bn_conv04_04_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv04_04_b"
    type: "Scale"
    bottom: "bn_conv04_04_b"
    top: "bn_conv04_04_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu04_04_b"
    type: "ReLU"
    bottom: "bn_conv04_04_b"
    top: "bn_conv04_04_b"
}
layer {
    name: "conv04_04_b"
    type: "Convolution"
    bottom: "bn_conv04_04_b"
    top: "conv04_04_b"
    param {
        name: "conv04_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv04_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise04_04"
    type: "Eltwise"
    bottom: "eltwise04_03"
    bottom: "conv04_04_b"
    top: "eltwise04_04"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv04_05_a"
    type: "BatchNorm"
    bottom: "eltwise04_04"
    top: "bn_conv04_05_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv04_05_a"
    type: "Scale"
    bottom: "bn_conv04_05_a"
    top: "bn_conv04_05_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu04_05_a"
    type: "ReLU"
    bottom: "bn_conv04_05_a"
    top: "bn_conv04_05_a"
}
layer {
    name: "conv04_05_a"
    type: "Convolution"
    bottom: "bn_conv04_05_a"
    top: "conv04_05_a"
    param {
        name: "conv04_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv04_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv04_05_b"
    type: "BatchNorm"
    bottom: "conv04_05_a"
    top: "bn_conv04_05_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv04_05_b"
    type: "Scale"
    bottom: "bn_conv04_05_b"
    top: "bn_conv04_05_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu04_05_b"
    type: "ReLU"
    bottom: "bn_conv04_05_b"
    top: "bn_conv04_05_b"
}
layer {
    name: "conv04_05_b"
    type: "Convolution"
    bottom: "bn_conv04_05_b"
    top: "conv04_05_b"
    param {
        name: "conv04_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv04_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise04_05"
    type: "Eltwise"
    bottom: "eltwise04_04"
    bottom: "conv04_05_b"
    top: "eltwise04_05"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv04_06_a"
    type: "BatchNorm"
    bottom: "eltwise04_05"
    top: "bn_conv04_06_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv04_06_a"
    type: "Scale"
    bottom: "bn_conv04_06_a"
    top: "bn_conv04_06_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu04_06_a"
    type: "ReLU"
    bottom: "bn_conv04_06_a"
    top: "bn_conv04_06_a"
}
layer {
    name: "conv04_06_a"
    type: "Convolution"
    bottom: "bn_conv04_06_a"
    top: "conv04_06_a"
    param {
        name: "conv04_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv04_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv04_06_b"
    type: "BatchNorm"
    bottom: "conv04_06_a"
    top: "bn_conv04_06_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv04_06_b"
    type: "Scale"
    bottom: "bn_conv04_06_b"
    top: "bn_conv04_06_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu04_06_b"
    type: "ReLU"
    bottom: "bn_conv04_06_b"
    top: "bn_conv04_06_b"
}
layer {
    name: "conv04_06_b"
    type: "Convolution"
    bottom: "bn_conv04_06_b"
    top: "conv04_06_b"
    param {
        name: "conv04_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv04_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise04_06"
    type: "Eltwise"
    bottom: "eltwise04_05"
    bottom: "conv04_06_b"
    top: "eltwise04_06"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv04_07_a"
    type: "BatchNorm"
    bottom: "eltwise04_06"
    top: "bn_conv04_07_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv04_07_a"
    type: "Scale"
    bottom: "bn_conv04_07_a"
    top: "bn_conv04_07_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu04_07_a"
    type: "ReLU"
    bottom: "bn_conv04_07_a"
    top: "bn_conv04_07_a"
}
layer {
    name: "conv04_07_a"
    type: "Convolution"
    bottom: "bn_conv04_07_a"
    top: "conv04_07_a"
    param {
        name: "conv04_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv04_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv04_07_b"
    type: "BatchNorm"
    bottom: "conv04_07_a"
    top: "bn_conv04_07_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv04_07_b"
    type: "Scale"
    bottom: "bn_conv04_07_b"
    top: "bn_conv04_07_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu04_07_b"
    type: "ReLU"
    bottom: "bn_conv04_07_b"
    top: "bn_conv04_07_b"
}
layer {
    name: "conv04_07_b"
    type: "Convolution"
    bottom: "bn_conv04_07_b"
    top: "conv04_07_b"
    param {
        name: "conv04_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv04_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise04_07"
    type: "Eltwise"
    bottom: "eltwise04_06"
    bottom: "conv04_07_b"
    top: "eltwise04_07"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv04_08_a"
    type: "BatchNorm"
    bottom: "eltwise04_07"
    top: "bn_conv04_08_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv04_08_a"
    type: "Scale"
    bottom: "bn_conv04_08_a"
    top: "bn_conv04_08_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu04_08_a"
    type: "ReLU"
    bottom: "bn_conv04_08_a"
    top: "bn_conv04_08_a"
}
layer {
    name: "conv04_08_a"
    type: "Convolution"
    bottom: "bn_conv04_08_a"
    top: "conv04_08_a"
    param {
        name: "conv04_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv04_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv04_08_b"
    type: "BatchNorm"
    bottom: "conv04_08_a"
    top: "bn_conv04_08_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv04_08_b"
    type: "Scale"
    bottom: "bn_conv04_08_b"
    top: "bn_conv04_08_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu04_08_b"
    type: "ReLU"
    bottom: "bn_conv04_08_b"
    top: "bn_conv04_08_b"
}
layer {
    name: "conv04_08_b"
    type: "Convolution"
    bottom: "bn_conv04_08_b"
    top: "conv04_08_b"
    param {
        name: "conv04_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv04_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise04_08"
    type: "Eltwise"
    bottom: "eltwise04_07"
    bottom: "conv04_08_b"
    top: "eltwise04_08"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv04_09_a"
    type: "BatchNorm"
    bottom: "eltwise04_08"
    top: "bn_conv04_09_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv04_09_a"
    type: "Scale"
    bottom: "bn_conv04_09_a"
    top: "bn_conv04_09_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu04_09_a"
    type: "ReLU"
    bottom: "bn_conv04_09_a"
    top: "bn_conv04_09_a"
}
layer {
    name: "conv04_09_a"
    type: "Convolution"
    bottom: "bn_conv04_09_a"
    top: "conv04_09_a"
    param {
        name: "conv04_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv04_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv04_09_b"
    type: "BatchNorm"
    bottom: "conv04_09_a"
    top: "bn_conv04_09_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv04_09_b"
    type: "Scale"
    bottom: "bn_conv04_09_b"
    top: "bn_conv04_09_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu04_09_b"
    type: "ReLU"
    bottom: "bn_conv04_09_b"
    top: "bn_conv04_09_b"
}
layer {
    name: "conv04_09_b"
    type: "Convolution"
    bottom: "bn_conv04_09_b"
    top: "conv04_09_b"
    param {
        name: "conv04_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv04_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise04_09"
    type: "Eltwise"
    bottom: "eltwise04_08"
    bottom: "conv04_09_b"
    top: "eltwise04_09"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv04_10_a"
    type: "BatchNorm"
    bottom: "eltwise04_09"
    top: "bn_conv04_10_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv04_10_a"
    type: "Scale"
    bottom: "bn_conv04_10_a"
    top: "bn_conv04_10_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu04_10_a"
    type: "ReLU"
    bottom: "bn_conv04_10_a"
    top: "bn_conv04_10_a"
}
layer {
    name: "conv04_10_a"
    type: "Convolution"
    bottom: "bn_conv04_10_a"
    top: "conv04_10_a"
    param {
        name: "conv04_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv04_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv04_10_b"
    type: "BatchNorm"
    bottom: "conv04_10_a"
    top: "bn_conv04_10_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv04_10_b"
    type: "Scale"
    bottom: "bn_conv04_10_b"
    top: "bn_conv04_10_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu04_10_b"
    type: "ReLU"
    bottom: "bn_conv04_10_b"
    top: "bn_conv04_10_b"
}
layer {
    name: "conv04_10_b"
    type: "Convolution"
    bottom: "bn_conv04_10_b"
    top: "conv04_10_b"
    param {
        name: "conv04_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv04_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise04_10"
    type: "Eltwise"
    bottom: "eltwise04_09"
    bottom: "conv04_10_b"
    top: "eltwise04_10"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "concat04"
    type: "Concat"
    bottom: "conv1"
    bottom: "conv_transition_01"
    bottom: "conv_transition_02"
    bottom: "conv_transition_03"
    bottom: "eltwise04_01"
    bottom: "eltwise04_02"
    bottom: "eltwise04_03"
    bottom: "eltwise04_04"
    bottom: "eltwise04_05"
    bottom: "eltwise04_06"
    bottom: "eltwise04_07"
    bottom: "eltwise04_08"
    bottom: "eltwise04_09"
    bottom: "eltwise04_10"
    top: "concat04"
    concat_param {
        axis: 1
    }
}
layer {
    name: "bn_conv_transition_04"
    type: "BatchNorm"
    bottom: "concat04"
    top: "bn_conv_transition_04"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv_transition_04"
    type: "Scale"
    bottom: "bn_conv_transition_04"
    top: "bn_conv_transition_04"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_transition_04"
    type: "ReLU"
    bottom: "bn_conv_transition_04"
    top: "bn_conv_transition_04"
}
layer {
    name: "conv_transition_04"
    type: "Convolution"
    bottom: "bn_conv_transition_04"
    top: "conv_transition_04"
    param {
        lr_mult: 1.000000
    }
    param {
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 1
        stride: 1
        pad: 0
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv05_01_a"
    type: "BatchNorm"
    bottom: "conv_transition_04"
    top: "bn_conv05_01_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv05_01_a"
    type: "Scale"
    bottom: "bn_conv05_01_a"
    top: "bn_conv05_01_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu05_01_a"
    type: "ReLU"
    bottom: "bn_conv05_01_a"
    top: "bn_conv05_01_a"
}
layer {
    name: "conv05_01_a"
    type: "Convolution"
    bottom: "bn_conv05_01_a"
    top: "conv05_01_a"
    param {
        name: "conv05_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv05_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv05_01_b"
    type: "BatchNorm"
    bottom: "conv05_01_a"
    top: "bn_conv05_01_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv05_01_b"
    type: "Scale"
    bottom: "bn_conv05_01_b"
    top: "bn_conv05_01_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu05_01_b"
    type: "ReLU"
    bottom: "bn_conv05_01_b"
    top: "bn_conv05_01_b"
}
layer {
    name: "conv05_01_b"
    type: "Convolution"
    bottom: "bn_conv05_01_b"
    top: "conv05_01_b"
    param {
        name: "conv05_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv05_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise05_01"
    type: "Eltwise"
    bottom: "conv_transition_04"
    bottom: "conv05_01_b"
    top: "eltwise05_01"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv05_02_a"
    type: "BatchNorm"
    bottom: "eltwise05_01"
    top: "bn_conv05_02_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv05_02_a"
    type: "Scale"
    bottom: "bn_conv05_02_a"
    top: "bn_conv05_02_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu05_02_a"
    type: "ReLU"
    bottom: "bn_conv05_02_a"
    top: "bn_conv05_02_a"
}
layer {
    name: "conv05_02_a"
    type: "Convolution"
    bottom: "bn_conv05_02_a"
    top: "conv05_02_a"
    param {
        name: "conv05_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv05_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv05_02_b"
    type: "BatchNorm"
    bottom: "conv05_02_a"
    top: "bn_conv05_02_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv05_02_b"
    type: "Scale"
    bottom: "bn_conv05_02_b"
    top: "bn_conv05_02_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu05_02_b"
    type: "ReLU"
    bottom: "bn_conv05_02_b"
    top: "bn_conv05_02_b"
}
layer {
    name: "conv05_02_b"
    type: "Convolution"
    bottom: "bn_conv05_02_b"
    top: "conv05_02_b"
    param {
        name: "conv05_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv05_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise05_02"
    type: "Eltwise"
    bottom: "eltwise05_01"
    bottom: "conv05_02_b"
    top: "eltwise05_02"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv05_03_a"
    type: "BatchNorm"
    bottom: "eltwise05_02"
    top: "bn_conv05_03_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv05_03_a"
    type: "Scale"
    bottom: "bn_conv05_03_a"
    top: "bn_conv05_03_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu05_03_a"
    type: "ReLU"
    bottom: "bn_conv05_03_a"
    top: "bn_conv05_03_a"
}
layer {
    name: "conv05_03_a"
    type: "Convolution"
    bottom: "bn_conv05_03_a"
    top: "conv05_03_a"
    param {
        name: "conv05_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv05_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv05_03_b"
    type: "BatchNorm"
    bottom: "conv05_03_a"
    top: "bn_conv05_03_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv05_03_b"
    type: "Scale"
    bottom: "bn_conv05_03_b"
    top: "bn_conv05_03_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu05_03_b"
    type: "ReLU"
    bottom: "bn_conv05_03_b"
    top: "bn_conv05_03_b"
}
layer {
    name: "conv05_03_b"
    type: "Convolution"
    bottom: "bn_conv05_03_b"
    top: "conv05_03_b"
    param {
        name: "conv05_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv05_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise05_03"
    type: "Eltwise"
    bottom: "eltwise05_02"
    bottom: "conv05_03_b"
    top: "eltwise05_03"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv05_04_a"
    type: "BatchNorm"
    bottom: "eltwise05_03"
    top: "bn_conv05_04_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv05_04_a"
    type: "Scale"
    bottom: "bn_conv05_04_a"
    top: "bn_conv05_04_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu05_04_a"
    type: "ReLU"
    bottom: "bn_conv05_04_a"
    top: "bn_conv05_04_a"
}
layer {
    name: "conv05_04_a"
    type: "Convolution"
    bottom: "bn_conv05_04_a"
    top: "conv05_04_a"
    param {
        name: "conv05_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv05_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv05_04_b"
    type: "BatchNorm"
    bottom: "conv05_04_a"
    top: "bn_conv05_04_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv05_04_b"
    type: "Scale"
    bottom: "bn_conv05_04_b"
    top: "bn_conv05_04_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu05_04_b"
    type: "ReLU"
    bottom: "bn_conv05_04_b"
    top: "bn_conv05_04_b"
}
layer {
    name: "conv05_04_b"
    type: "Convolution"
    bottom: "bn_conv05_04_b"
    top: "conv05_04_b"
    param {
        name: "conv05_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv05_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise05_04"
    type: "Eltwise"
    bottom: "eltwise05_03"
    bottom: "conv05_04_b"
    top: "eltwise05_04"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv05_05_a"
    type: "BatchNorm"
    bottom: "eltwise05_04"
    top: "bn_conv05_05_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv05_05_a"
    type: "Scale"
    bottom: "bn_conv05_05_a"
    top: "bn_conv05_05_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu05_05_a"
    type: "ReLU"
    bottom: "bn_conv05_05_a"
    top: "bn_conv05_05_a"
}
layer {
    name: "conv05_05_a"
    type: "Convolution"
    bottom: "bn_conv05_05_a"
    top: "conv05_05_a"
    param {
        name: "conv05_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv05_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv05_05_b"
    type: "BatchNorm"
    bottom: "conv05_05_a"
    top: "bn_conv05_05_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv05_05_b"
    type: "Scale"
    bottom: "bn_conv05_05_b"
    top: "bn_conv05_05_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu05_05_b"
    type: "ReLU"
    bottom: "bn_conv05_05_b"
    top: "bn_conv05_05_b"
}
layer {
    name: "conv05_05_b"
    type: "Convolution"
    bottom: "bn_conv05_05_b"
    top: "conv05_05_b"
    param {
        name: "conv05_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv05_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise05_05"
    type: "Eltwise"
    bottom: "eltwise05_04"
    bottom: "conv05_05_b"
    top: "eltwise05_05"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv05_06_a"
    type: "BatchNorm"
    bottom: "eltwise05_05"
    top: "bn_conv05_06_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv05_06_a"
    type: "Scale"
    bottom: "bn_conv05_06_a"
    top: "bn_conv05_06_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu05_06_a"
    type: "ReLU"
    bottom: "bn_conv05_06_a"
    top: "bn_conv05_06_a"
}
layer {
    name: "conv05_06_a"
    type: "Convolution"
    bottom: "bn_conv05_06_a"
    top: "conv05_06_a"
    param {
        name: "conv05_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv05_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv05_06_b"
    type: "BatchNorm"
    bottom: "conv05_06_a"
    top: "bn_conv05_06_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv05_06_b"
    type: "Scale"
    bottom: "bn_conv05_06_b"
    top: "bn_conv05_06_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu05_06_b"
    type: "ReLU"
    bottom: "bn_conv05_06_b"
    top: "bn_conv05_06_b"
}
layer {
    name: "conv05_06_b"
    type: "Convolution"
    bottom: "bn_conv05_06_b"
    top: "conv05_06_b"
    param {
        name: "conv05_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv05_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise05_06"
    type: "Eltwise"
    bottom: "eltwise05_05"
    bottom: "conv05_06_b"
    top: "eltwise05_06"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv05_07_a"
    type: "BatchNorm"
    bottom: "eltwise05_06"
    top: "bn_conv05_07_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv05_07_a"
    type: "Scale"
    bottom: "bn_conv05_07_a"
    top: "bn_conv05_07_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu05_07_a"
    type: "ReLU"
    bottom: "bn_conv05_07_a"
    top: "bn_conv05_07_a"
}
layer {
    name: "conv05_07_a"
    type: "Convolution"
    bottom: "bn_conv05_07_a"
    top: "conv05_07_a"
    param {
        name: "conv05_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv05_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv05_07_b"
    type: "BatchNorm"
    bottom: "conv05_07_a"
    top: "bn_conv05_07_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv05_07_b"
    type: "Scale"
    bottom: "bn_conv05_07_b"
    top: "bn_conv05_07_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu05_07_b"
    type: "ReLU"
    bottom: "bn_conv05_07_b"
    top: "bn_conv05_07_b"
}
layer {
    name: "conv05_07_b"
    type: "Convolution"
    bottom: "bn_conv05_07_b"
    top: "conv05_07_b"
    param {
        name: "conv05_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv05_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise05_07"
    type: "Eltwise"
    bottom: "eltwise05_06"
    bottom: "conv05_07_b"
    top: "eltwise05_07"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv05_08_a"
    type: "BatchNorm"
    bottom: "eltwise05_07"
    top: "bn_conv05_08_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv05_08_a"
    type: "Scale"
    bottom: "bn_conv05_08_a"
    top: "bn_conv05_08_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu05_08_a"
    type: "ReLU"
    bottom: "bn_conv05_08_a"
    top: "bn_conv05_08_a"
}
layer {
    name: "conv05_08_a"
    type: "Convolution"
    bottom: "bn_conv05_08_a"
    top: "conv05_08_a"
    param {
        name: "conv05_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv05_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv05_08_b"
    type: "BatchNorm"
    bottom: "conv05_08_a"
    top: "bn_conv05_08_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv05_08_b"
    type: "Scale"
    bottom: "bn_conv05_08_b"
    top: "bn_conv05_08_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu05_08_b"
    type: "ReLU"
    bottom: "bn_conv05_08_b"
    top: "bn_conv05_08_b"
}
layer {
    name: "conv05_08_b"
    type: "Convolution"
    bottom: "bn_conv05_08_b"
    top: "conv05_08_b"
    param {
        name: "conv05_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv05_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise05_08"
    type: "Eltwise"
    bottom: "eltwise05_07"
    bottom: "conv05_08_b"
    top: "eltwise05_08"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv05_09_a"
    type: "BatchNorm"
    bottom: "eltwise05_08"
    top: "bn_conv05_09_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv05_09_a"
    type: "Scale"
    bottom: "bn_conv05_09_a"
    top: "bn_conv05_09_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu05_09_a"
    type: "ReLU"
    bottom: "bn_conv05_09_a"
    top: "bn_conv05_09_a"
}
layer {
    name: "conv05_09_a"
    type: "Convolution"
    bottom: "bn_conv05_09_a"
    top: "conv05_09_a"
    param {
        name: "conv05_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv05_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv05_09_b"
    type: "BatchNorm"
    bottom: "conv05_09_a"
    top: "bn_conv05_09_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv05_09_b"
    type: "Scale"
    bottom: "bn_conv05_09_b"
    top: "bn_conv05_09_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu05_09_b"
    type: "ReLU"
    bottom: "bn_conv05_09_b"
    top: "bn_conv05_09_b"
}
layer {
    name: "conv05_09_b"
    type: "Convolution"
    bottom: "bn_conv05_09_b"
    top: "conv05_09_b"
    param {
        name: "conv05_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv05_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise05_09"
    type: "Eltwise"
    bottom: "eltwise05_08"
    bottom: "conv05_09_b"
    top: "eltwise05_09"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv05_10_a"
    type: "BatchNorm"
    bottom: "eltwise05_09"
    top: "bn_conv05_10_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv05_10_a"
    type: "Scale"
    bottom: "bn_conv05_10_a"
    top: "bn_conv05_10_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu05_10_a"
    type: "ReLU"
    bottom: "bn_conv05_10_a"
    top: "bn_conv05_10_a"
}
layer {
    name: "conv05_10_a"
    type: "Convolution"
    bottom: "bn_conv05_10_a"
    top: "conv05_10_a"
    param {
        name: "conv05_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv05_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv05_10_b"
    type: "BatchNorm"
    bottom: "conv05_10_a"
    top: "bn_conv05_10_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv05_10_b"
    type: "Scale"
    bottom: "bn_conv05_10_b"
    top: "bn_conv05_10_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu05_10_b"
    type: "ReLU"
    bottom: "bn_conv05_10_b"
    top: "bn_conv05_10_b"
}
layer {
    name: "conv05_10_b"
    type: "Convolution"
    bottom: "bn_conv05_10_b"
    top: "conv05_10_b"
    param {
        name: "conv05_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv05_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise05_10"
    type: "Eltwise"
    bottom: "eltwise05_09"
    bottom: "conv05_10_b"
    top: "eltwise05_10"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "concat05"
    type: "Concat"
    bottom: "conv1"
    bottom: "conv_transition_01"
    bottom: "conv_transition_02"
    bottom: "conv_transition_03"
    bottom: "conv_transition_04"
    bottom: "eltwise05_01"
    bottom: "eltwise05_02"
    bottom: "eltwise05_03"
    bottom: "eltwise05_04"
    bottom: "eltwise05_05"
    bottom: "eltwise05_06"
    bottom: "eltwise05_07"
    bottom: "eltwise05_08"
    bottom: "eltwise05_09"
    bottom: "eltwise05_10"
    top: "concat05"
    concat_param {
        axis: 1
    }
}
layer {
    name: "bn_conv_transition_05"
    type: "BatchNorm"
    bottom: "concat05"
    top: "bn_conv_transition_05"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv_transition_05"
    type: "Scale"
    bottom: "bn_conv_transition_05"
    top: "bn_conv_transition_05"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_transition_05"
    type: "ReLU"
    bottom: "bn_conv_transition_05"
    top: "bn_conv_transition_05"
}
layer {
    name: "conv_transition_05"
    type: "Convolution"
    bottom: "bn_conv_transition_05"
    top: "conv_transition_05"
    param {
        lr_mult: 1.000000
    }
    param {
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 1
        stride: 1
        pad: 0
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv06_01_a"
    type: "BatchNorm"
    bottom: "conv_transition_05"
    top: "bn_conv06_01_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv06_01_a"
    type: "Scale"
    bottom: "bn_conv06_01_a"
    top: "bn_conv06_01_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu06_01_a"
    type: "ReLU"
    bottom: "bn_conv06_01_a"
    top: "bn_conv06_01_a"
}
layer {
    name: "conv06_01_a"
    type: "Convolution"
    bottom: "bn_conv06_01_a"
    top: "conv06_01_a"
    param {
        name: "conv06_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv06_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv06_01_b"
    type: "BatchNorm"
    bottom: "conv06_01_a"
    top: "bn_conv06_01_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv06_01_b"
    type: "Scale"
    bottom: "bn_conv06_01_b"
    top: "bn_conv06_01_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu06_01_b"
    type: "ReLU"
    bottom: "bn_conv06_01_b"
    top: "bn_conv06_01_b"
}
layer {
    name: "conv06_01_b"
    type: "Convolution"
    bottom: "bn_conv06_01_b"
    top: "conv06_01_b"
    param {
        name: "conv06_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv06_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise06_01"
    type: "Eltwise"
    bottom: "conv_transition_05"
    bottom: "conv06_01_b"
    top: "eltwise06_01"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv06_02_a"
    type: "BatchNorm"
    bottom: "eltwise06_01"
    top: "bn_conv06_02_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv06_02_a"
    type: "Scale"
    bottom: "bn_conv06_02_a"
    top: "bn_conv06_02_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu06_02_a"
    type: "ReLU"
    bottom: "bn_conv06_02_a"
    top: "bn_conv06_02_a"
}
layer {
    name: "conv06_02_a"
    type: "Convolution"
    bottom: "bn_conv06_02_a"
    top: "conv06_02_a"
    param {
        name: "conv06_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv06_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv06_02_b"
    type: "BatchNorm"
    bottom: "conv06_02_a"
    top: "bn_conv06_02_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv06_02_b"
    type: "Scale"
    bottom: "bn_conv06_02_b"
    top: "bn_conv06_02_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu06_02_b"
    type: "ReLU"
    bottom: "bn_conv06_02_b"
    top: "bn_conv06_02_b"
}
layer {
    name: "conv06_02_b"
    type: "Convolution"
    bottom: "bn_conv06_02_b"
    top: "conv06_02_b"
    param {
        name: "conv06_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv06_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise06_02"
    type: "Eltwise"
    bottom: "eltwise06_01"
    bottom: "conv06_02_b"
    top: "eltwise06_02"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv06_03_a"
    type: "BatchNorm"
    bottom: "eltwise06_02"
    top: "bn_conv06_03_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv06_03_a"
    type: "Scale"
    bottom: "bn_conv06_03_a"
    top: "bn_conv06_03_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu06_03_a"
    type: "ReLU"
    bottom: "bn_conv06_03_a"
    top: "bn_conv06_03_a"
}
layer {
    name: "conv06_03_a"
    type: "Convolution"
    bottom: "bn_conv06_03_a"
    top: "conv06_03_a"
    param {
        name: "conv06_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv06_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv06_03_b"
    type: "BatchNorm"
    bottom: "conv06_03_a"
    top: "bn_conv06_03_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv06_03_b"
    type: "Scale"
    bottom: "bn_conv06_03_b"
    top: "bn_conv06_03_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu06_03_b"
    type: "ReLU"
    bottom: "bn_conv06_03_b"
    top: "bn_conv06_03_b"
}
layer {
    name: "conv06_03_b"
    type: "Convolution"
    bottom: "bn_conv06_03_b"
    top: "conv06_03_b"
    param {
        name: "conv06_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv06_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise06_03"
    type: "Eltwise"
    bottom: "eltwise06_02"
    bottom: "conv06_03_b"
    top: "eltwise06_03"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv06_04_a"
    type: "BatchNorm"
    bottom: "eltwise06_03"
    top: "bn_conv06_04_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv06_04_a"
    type: "Scale"
    bottom: "bn_conv06_04_a"
    top: "bn_conv06_04_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu06_04_a"
    type: "ReLU"
    bottom: "bn_conv06_04_a"
    top: "bn_conv06_04_a"
}
layer {
    name: "conv06_04_a"
    type: "Convolution"
    bottom: "bn_conv06_04_a"
    top: "conv06_04_a"
    param {
        name: "conv06_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv06_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv06_04_b"
    type: "BatchNorm"
    bottom: "conv06_04_a"
    top: "bn_conv06_04_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv06_04_b"
    type: "Scale"
    bottom: "bn_conv06_04_b"
    top: "bn_conv06_04_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu06_04_b"
    type: "ReLU"
    bottom: "bn_conv06_04_b"
    top: "bn_conv06_04_b"
}
layer {
    name: "conv06_04_b"
    type: "Convolution"
    bottom: "bn_conv06_04_b"
    top: "conv06_04_b"
    param {
        name: "conv06_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv06_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise06_04"
    type: "Eltwise"
    bottom: "eltwise06_03"
    bottom: "conv06_04_b"
    top: "eltwise06_04"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv06_05_a"
    type: "BatchNorm"
    bottom: "eltwise06_04"
    top: "bn_conv06_05_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv06_05_a"
    type: "Scale"
    bottom: "bn_conv06_05_a"
    top: "bn_conv06_05_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu06_05_a"
    type: "ReLU"
    bottom: "bn_conv06_05_a"
    top: "bn_conv06_05_a"
}
layer {
    name: "conv06_05_a"
    type: "Convolution"
    bottom: "bn_conv06_05_a"
    top: "conv06_05_a"
    param {
        name: "conv06_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv06_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv06_05_b"
    type: "BatchNorm"
    bottom: "conv06_05_a"
    top: "bn_conv06_05_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv06_05_b"
    type: "Scale"
    bottom: "bn_conv06_05_b"
    top: "bn_conv06_05_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu06_05_b"
    type: "ReLU"
    bottom: "bn_conv06_05_b"
    top: "bn_conv06_05_b"
}
layer {
    name: "conv06_05_b"
    type: "Convolution"
    bottom: "bn_conv06_05_b"
    top: "conv06_05_b"
    param {
        name: "conv06_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv06_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise06_05"
    type: "Eltwise"
    bottom: "eltwise06_04"
    bottom: "conv06_05_b"
    top: "eltwise06_05"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv06_06_a"
    type: "BatchNorm"
    bottom: "eltwise06_05"
    top: "bn_conv06_06_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv06_06_a"
    type: "Scale"
    bottom: "bn_conv06_06_a"
    top: "bn_conv06_06_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu06_06_a"
    type: "ReLU"
    bottom: "bn_conv06_06_a"
    top: "bn_conv06_06_a"
}
layer {
    name: "conv06_06_a"
    type: "Convolution"
    bottom: "bn_conv06_06_a"
    top: "conv06_06_a"
    param {
        name: "conv06_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv06_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv06_06_b"
    type: "BatchNorm"
    bottom: "conv06_06_a"
    top: "bn_conv06_06_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv06_06_b"
    type: "Scale"
    bottom: "bn_conv06_06_b"
    top: "bn_conv06_06_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu06_06_b"
    type: "ReLU"
    bottom: "bn_conv06_06_b"
    top: "bn_conv06_06_b"
}
layer {
    name: "conv06_06_b"
    type: "Convolution"
    bottom: "bn_conv06_06_b"
    top: "conv06_06_b"
    param {
        name: "conv06_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv06_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise06_06"
    type: "Eltwise"
    bottom: "eltwise06_05"
    bottom: "conv06_06_b"
    top: "eltwise06_06"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv06_07_a"
    type: "BatchNorm"
    bottom: "eltwise06_06"
    top: "bn_conv06_07_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv06_07_a"
    type: "Scale"
    bottom: "bn_conv06_07_a"
    top: "bn_conv06_07_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu06_07_a"
    type: "ReLU"
    bottom: "bn_conv06_07_a"
    top: "bn_conv06_07_a"
}
layer {
    name: "conv06_07_a"
    type: "Convolution"
    bottom: "bn_conv06_07_a"
    top: "conv06_07_a"
    param {
        name: "conv06_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv06_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv06_07_b"
    type: "BatchNorm"
    bottom: "conv06_07_a"
    top: "bn_conv06_07_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv06_07_b"
    type: "Scale"
    bottom: "bn_conv06_07_b"
    top: "bn_conv06_07_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu06_07_b"
    type: "ReLU"
    bottom: "bn_conv06_07_b"
    top: "bn_conv06_07_b"
}
layer {
    name: "conv06_07_b"
    type: "Convolution"
    bottom: "bn_conv06_07_b"
    top: "conv06_07_b"
    param {
        name: "conv06_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv06_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise06_07"
    type: "Eltwise"
    bottom: "eltwise06_06"
    bottom: "conv06_07_b"
    top: "eltwise06_07"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv06_08_a"
    type: "BatchNorm"
    bottom: "eltwise06_07"
    top: "bn_conv06_08_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv06_08_a"
    type: "Scale"
    bottom: "bn_conv06_08_a"
    top: "bn_conv06_08_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu06_08_a"
    type: "ReLU"
    bottom: "bn_conv06_08_a"
    top: "bn_conv06_08_a"
}
layer {
    name: "conv06_08_a"
    type: "Convolution"
    bottom: "bn_conv06_08_a"
    top: "conv06_08_a"
    param {
        name: "conv06_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv06_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv06_08_b"
    type: "BatchNorm"
    bottom: "conv06_08_a"
    top: "bn_conv06_08_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv06_08_b"
    type: "Scale"
    bottom: "bn_conv06_08_b"
    top: "bn_conv06_08_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu06_08_b"
    type: "ReLU"
    bottom: "bn_conv06_08_b"
    top: "bn_conv06_08_b"
}
layer {
    name: "conv06_08_b"
    type: "Convolution"
    bottom: "bn_conv06_08_b"
    top: "conv06_08_b"
    param {
        name: "conv06_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv06_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise06_08"
    type: "Eltwise"
    bottom: "eltwise06_07"
    bottom: "conv06_08_b"
    top: "eltwise06_08"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv06_09_a"
    type: "BatchNorm"
    bottom: "eltwise06_08"
    top: "bn_conv06_09_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv06_09_a"
    type: "Scale"
    bottom: "bn_conv06_09_a"
    top: "bn_conv06_09_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu06_09_a"
    type: "ReLU"
    bottom: "bn_conv06_09_a"
    top: "bn_conv06_09_a"
}
layer {
    name: "conv06_09_a"
    type: "Convolution"
    bottom: "bn_conv06_09_a"
    top: "conv06_09_a"
    param {
        name: "conv06_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv06_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv06_09_b"
    type: "BatchNorm"
    bottom: "conv06_09_a"
    top: "bn_conv06_09_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv06_09_b"
    type: "Scale"
    bottom: "bn_conv06_09_b"
    top: "bn_conv06_09_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu06_09_b"
    type: "ReLU"
    bottom: "bn_conv06_09_b"
    top: "bn_conv06_09_b"
}
layer {
    name: "conv06_09_b"
    type: "Convolution"
    bottom: "bn_conv06_09_b"
    top: "conv06_09_b"
    param {
        name: "conv06_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv06_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise06_09"
    type: "Eltwise"
    bottom: "eltwise06_08"
    bottom: "conv06_09_b"
    top: "eltwise06_09"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv06_10_a"
    type: "BatchNorm"
    bottom: "eltwise06_09"
    top: "bn_conv06_10_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv06_10_a"
    type: "Scale"
    bottom: "bn_conv06_10_a"
    top: "bn_conv06_10_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu06_10_a"
    type: "ReLU"
    bottom: "bn_conv06_10_a"
    top: "bn_conv06_10_a"
}
layer {
    name: "conv06_10_a"
    type: "Convolution"
    bottom: "bn_conv06_10_a"
    top: "conv06_10_a"
    param {
        name: "conv06_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv06_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv06_10_b"
    type: "BatchNorm"
    bottom: "conv06_10_a"
    top: "bn_conv06_10_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv06_10_b"
    type: "Scale"
    bottom: "bn_conv06_10_b"
    top: "bn_conv06_10_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu06_10_b"
    type: "ReLU"
    bottom: "bn_conv06_10_b"
    top: "bn_conv06_10_b"
}
layer {
    name: "conv06_10_b"
    type: "Convolution"
    bottom: "bn_conv06_10_b"
    top: "conv06_10_b"
    param {
        name: "conv06_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv06_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise06_10"
    type: "Eltwise"
    bottom: "eltwise06_09"
    bottom: "conv06_10_b"
    top: "eltwise06_10"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "concat06"
    type: "Concat"
    bottom: "conv1"
    bottom: "conv_transition_01"
    bottom: "conv_transition_02"
    bottom: "conv_transition_03"
    bottom: "conv_transition_04"
    bottom: "conv_transition_05"
    bottom: "eltwise06_01"
    bottom: "eltwise06_02"
    bottom: "eltwise06_03"
    bottom: "eltwise06_04"
    bottom: "eltwise06_05"
    bottom: "eltwise06_06"
    bottom: "eltwise06_07"
    bottom: "eltwise06_08"
    bottom: "eltwise06_09"
    bottom: "eltwise06_10"
    top: "concat06"
    concat_param {
        axis: 1
    }
}
layer {
    name: "bn_conv_transition_06"
    type: "BatchNorm"
    bottom: "concat06"
    top: "bn_conv_transition_06"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv_transition_06"
    type: "Scale"
    bottom: "bn_conv_transition_06"
    top: "bn_conv_transition_06"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_transition_06"
    type: "ReLU"
    bottom: "bn_conv_transition_06"
    top: "bn_conv_transition_06"
}
layer {
    name: "conv_transition_06"
    type: "Convolution"
    bottom: "bn_conv_transition_06"
    top: "conv_transition_06"
    param {
        lr_mult: 1.000000
    }
    param {
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 1
        stride: 1
        pad: 0
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv07_01_a"
    type: "BatchNorm"
    bottom: "conv_transition_06"
    top: "bn_conv07_01_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv07_01_a"
    type: "Scale"
    bottom: "bn_conv07_01_a"
    top: "bn_conv07_01_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu07_01_a"
    type: "ReLU"
    bottom: "bn_conv07_01_a"
    top: "bn_conv07_01_a"
}
layer {
    name: "conv07_01_a"
    type: "Convolution"
    bottom: "bn_conv07_01_a"
    top: "conv07_01_a"
    param {
        name: "conv07_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv07_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv07_01_b"
    type: "BatchNorm"
    bottom: "conv07_01_a"
    top: "bn_conv07_01_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv07_01_b"
    type: "Scale"
    bottom: "bn_conv07_01_b"
    top: "bn_conv07_01_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu07_01_b"
    type: "ReLU"
    bottom: "bn_conv07_01_b"
    top: "bn_conv07_01_b"
}
layer {
    name: "conv07_01_b"
    type: "Convolution"
    bottom: "bn_conv07_01_b"
    top: "conv07_01_b"
    param {
        name: "conv07_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv07_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise07_01"
    type: "Eltwise"
    bottom: "conv_transition_06"
    bottom: "conv07_01_b"
    top: "eltwise07_01"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv07_02_a"
    type: "BatchNorm"
    bottom: "eltwise07_01"
    top: "bn_conv07_02_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv07_02_a"
    type: "Scale"
    bottom: "bn_conv07_02_a"
    top: "bn_conv07_02_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu07_02_a"
    type: "ReLU"
    bottom: "bn_conv07_02_a"
    top: "bn_conv07_02_a"
}
layer {
    name: "conv07_02_a"
    type: "Convolution"
    bottom: "bn_conv07_02_a"
    top: "conv07_02_a"
    param {
        name: "conv07_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv07_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv07_02_b"
    type: "BatchNorm"
    bottom: "conv07_02_a"
    top: "bn_conv07_02_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv07_02_b"
    type: "Scale"
    bottom: "bn_conv07_02_b"
    top: "bn_conv07_02_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu07_02_b"
    type: "ReLU"
    bottom: "bn_conv07_02_b"
    top: "bn_conv07_02_b"
}
layer {
    name: "conv07_02_b"
    type: "Convolution"
    bottom: "bn_conv07_02_b"
    top: "conv07_02_b"
    param {
        name: "conv07_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv07_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise07_02"
    type: "Eltwise"
    bottom: "eltwise07_01"
    bottom: "conv07_02_b"
    top: "eltwise07_02"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv07_03_a"
    type: "BatchNorm"
    bottom: "eltwise07_02"
    top: "bn_conv07_03_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv07_03_a"
    type: "Scale"
    bottom: "bn_conv07_03_a"
    top: "bn_conv07_03_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu07_03_a"
    type: "ReLU"
    bottom: "bn_conv07_03_a"
    top: "bn_conv07_03_a"
}
layer {
    name: "conv07_03_a"
    type: "Convolution"
    bottom: "bn_conv07_03_a"
    top: "conv07_03_a"
    param {
        name: "conv07_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv07_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv07_03_b"
    type: "BatchNorm"
    bottom: "conv07_03_a"
    top: "bn_conv07_03_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv07_03_b"
    type: "Scale"
    bottom: "bn_conv07_03_b"
    top: "bn_conv07_03_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu07_03_b"
    type: "ReLU"
    bottom: "bn_conv07_03_b"
    top: "bn_conv07_03_b"
}
layer {
    name: "conv07_03_b"
    type: "Convolution"
    bottom: "bn_conv07_03_b"
    top: "conv07_03_b"
    param {
        name: "conv07_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv07_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise07_03"
    type: "Eltwise"
    bottom: "eltwise07_02"
    bottom: "conv07_03_b"
    top: "eltwise07_03"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv07_04_a"
    type: "BatchNorm"
    bottom: "eltwise07_03"
    top: "bn_conv07_04_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv07_04_a"
    type: "Scale"
    bottom: "bn_conv07_04_a"
    top: "bn_conv07_04_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu07_04_a"
    type: "ReLU"
    bottom: "bn_conv07_04_a"
    top: "bn_conv07_04_a"
}
layer {
    name: "conv07_04_a"
    type: "Convolution"
    bottom: "bn_conv07_04_a"
    top: "conv07_04_a"
    param {
        name: "conv07_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv07_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv07_04_b"
    type: "BatchNorm"
    bottom: "conv07_04_a"
    top: "bn_conv07_04_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv07_04_b"
    type: "Scale"
    bottom: "bn_conv07_04_b"
    top: "bn_conv07_04_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu07_04_b"
    type: "ReLU"
    bottom: "bn_conv07_04_b"
    top: "bn_conv07_04_b"
}
layer {
    name: "conv07_04_b"
    type: "Convolution"
    bottom: "bn_conv07_04_b"
    top: "conv07_04_b"
    param {
        name: "conv07_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv07_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise07_04"
    type: "Eltwise"
    bottom: "eltwise07_03"
    bottom: "conv07_04_b"
    top: "eltwise07_04"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv07_05_a"
    type: "BatchNorm"
    bottom: "eltwise07_04"
    top: "bn_conv07_05_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv07_05_a"
    type: "Scale"
    bottom: "bn_conv07_05_a"
    top: "bn_conv07_05_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu07_05_a"
    type: "ReLU"
    bottom: "bn_conv07_05_a"
    top: "bn_conv07_05_a"
}
layer {
    name: "conv07_05_a"
    type: "Convolution"
    bottom: "bn_conv07_05_a"
    top: "conv07_05_a"
    param {
        name: "conv07_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv07_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv07_05_b"
    type: "BatchNorm"
    bottom: "conv07_05_a"
    top: "bn_conv07_05_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv07_05_b"
    type: "Scale"
    bottom: "bn_conv07_05_b"
    top: "bn_conv07_05_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu07_05_b"
    type: "ReLU"
    bottom: "bn_conv07_05_b"
    top: "bn_conv07_05_b"
}
layer {
    name: "conv07_05_b"
    type: "Convolution"
    bottom: "bn_conv07_05_b"
    top: "conv07_05_b"
    param {
        name: "conv07_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv07_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise07_05"
    type: "Eltwise"
    bottom: "eltwise07_04"
    bottom: "conv07_05_b"
    top: "eltwise07_05"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv07_06_a"
    type: "BatchNorm"
    bottom: "eltwise07_05"
    top: "bn_conv07_06_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv07_06_a"
    type: "Scale"
    bottom: "bn_conv07_06_a"
    top: "bn_conv07_06_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu07_06_a"
    type: "ReLU"
    bottom: "bn_conv07_06_a"
    top: "bn_conv07_06_a"
}
layer {
    name: "conv07_06_a"
    type: "Convolution"
    bottom: "bn_conv07_06_a"
    top: "conv07_06_a"
    param {
        name: "conv07_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv07_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv07_06_b"
    type: "BatchNorm"
    bottom: "conv07_06_a"
    top: "bn_conv07_06_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv07_06_b"
    type: "Scale"
    bottom: "bn_conv07_06_b"
    top: "bn_conv07_06_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu07_06_b"
    type: "ReLU"
    bottom: "bn_conv07_06_b"
    top: "bn_conv07_06_b"
}
layer {
    name: "conv07_06_b"
    type: "Convolution"
    bottom: "bn_conv07_06_b"
    top: "conv07_06_b"
    param {
        name: "conv07_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv07_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise07_06"
    type: "Eltwise"
    bottom: "eltwise07_05"
    bottom: "conv07_06_b"
    top: "eltwise07_06"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv07_07_a"
    type: "BatchNorm"
    bottom: "eltwise07_06"
    top: "bn_conv07_07_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv07_07_a"
    type: "Scale"
    bottom: "bn_conv07_07_a"
    top: "bn_conv07_07_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu07_07_a"
    type: "ReLU"
    bottom: "bn_conv07_07_a"
    top: "bn_conv07_07_a"
}
layer {
    name: "conv07_07_a"
    type: "Convolution"
    bottom: "bn_conv07_07_a"
    top: "conv07_07_a"
    param {
        name: "conv07_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv07_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv07_07_b"
    type: "BatchNorm"
    bottom: "conv07_07_a"
    top: "bn_conv07_07_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv07_07_b"
    type: "Scale"
    bottom: "bn_conv07_07_b"
    top: "bn_conv07_07_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu07_07_b"
    type: "ReLU"
    bottom: "bn_conv07_07_b"
    top: "bn_conv07_07_b"
}
layer {
    name: "conv07_07_b"
    type: "Convolution"
    bottom: "bn_conv07_07_b"
    top: "conv07_07_b"
    param {
        name: "conv07_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv07_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise07_07"
    type: "Eltwise"
    bottom: "eltwise07_06"
    bottom: "conv07_07_b"
    top: "eltwise07_07"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv07_08_a"
    type: "BatchNorm"
    bottom: "eltwise07_07"
    top: "bn_conv07_08_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv07_08_a"
    type: "Scale"
    bottom: "bn_conv07_08_a"
    top: "bn_conv07_08_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu07_08_a"
    type: "ReLU"
    bottom: "bn_conv07_08_a"
    top: "bn_conv07_08_a"
}
layer {
    name: "conv07_08_a"
    type: "Convolution"
    bottom: "bn_conv07_08_a"
    top: "conv07_08_a"
    param {
        name: "conv07_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv07_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv07_08_b"
    type: "BatchNorm"
    bottom: "conv07_08_a"
    top: "bn_conv07_08_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv07_08_b"
    type: "Scale"
    bottom: "bn_conv07_08_b"
    top: "bn_conv07_08_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu07_08_b"
    type: "ReLU"
    bottom: "bn_conv07_08_b"
    top: "bn_conv07_08_b"
}
layer {
    name: "conv07_08_b"
    type: "Convolution"
    bottom: "bn_conv07_08_b"
    top: "conv07_08_b"
    param {
        name: "conv07_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv07_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise07_08"
    type: "Eltwise"
    bottom: "eltwise07_07"
    bottom: "conv07_08_b"
    top: "eltwise07_08"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv07_09_a"
    type: "BatchNorm"
    bottom: "eltwise07_08"
    top: "bn_conv07_09_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv07_09_a"
    type: "Scale"
    bottom: "bn_conv07_09_a"
    top: "bn_conv07_09_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu07_09_a"
    type: "ReLU"
    bottom: "bn_conv07_09_a"
    top: "bn_conv07_09_a"
}
layer {
    name: "conv07_09_a"
    type: "Convolution"
    bottom: "bn_conv07_09_a"
    top: "conv07_09_a"
    param {
        name: "conv07_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv07_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv07_09_b"
    type: "BatchNorm"
    bottom: "conv07_09_a"
    top: "bn_conv07_09_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv07_09_b"
    type: "Scale"
    bottom: "bn_conv07_09_b"
    top: "bn_conv07_09_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu07_09_b"
    type: "ReLU"
    bottom: "bn_conv07_09_b"
    top: "bn_conv07_09_b"
}
layer {
    name: "conv07_09_b"
    type: "Convolution"
    bottom: "bn_conv07_09_b"
    top: "conv07_09_b"
    param {
        name: "conv07_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv07_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise07_09"
    type: "Eltwise"
    bottom: "eltwise07_08"
    bottom: "conv07_09_b"
    top: "eltwise07_09"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv07_10_a"
    type: "BatchNorm"
    bottom: "eltwise07_09"
    top: "bn_conv07_10_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv07_10_a"
    type: "Scale"
    bottom: "bn_conv07_10_a"
    top: "bn_conv07_10_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu07_10_a"
    type: "ReLU"
    bottom: "bn_conv07_10_a"
    top: "bn_conv07_10_a"
}
layer {
    name: "conv07_10_a"
    type: "Convolution"
    bottom: "bn_conv07_10_a"
    top: "conv07_10_a"
    param {
        name: "conv07_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv07_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv07_10_b"
    type: "BatchNorm"
    bottom: "conv07_10_a"
    top: "bn_conv07_10_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv07_10_b"
    type: "Scale"
    bottom: "bn_conv07_10_b"
    top: "bn_conv07_10_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu07_10_b"
    type: "ReLU"
    bottom: "bn_conv07_10_b"
    top: "bn_conv07_10_b"
}
layer {
    name: "conv07_10_b"
    type: "Convolution"
    bottom: "bn_conv07_10_b"
    top: "conv07_10_b"
    param {
        name: "conv07_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv07_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise07_10"
    type: "Eltwise"
    bottom: "eltwise07_09"
    bottom: "conv07_10_b"
    top: "eltwise07_10"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "concat07"
    type: "Concat"
    bottom: "conv1"
    bottom: "conv_transition_01"
    bottom: "conv_transition_02"
    bottom: "conv_transition_03"
    bottom: "conv_transition_04"
    bottom: "conv_transition_05"
    bottom: "conv_transition_06"
    bottom: "eltwise07_01"
    bottom: "eltwise07_02"
    bottom: "eltwise07_03"
    bottom: "eltwise07_04"
    bottom: "eltwise07_05"
    bottom: "eltwise07_06"
    bottom: "eltwise07_07"
    bottom: "eltwise07_08"
    bottom: "eltwise07_09"
    bottom: "eltwise07_10"
    top: "concat07"
    concat_param {
        axis: 1
    }
}
layer {
    name: "bn_conv_transition_07"
    type: "BatchNorm"
    bottom: "concat07"
    top: "bn_conv_transition_07"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv_transition_07"
    type: "Scale"
    bottom: "bn_conv_transition_07"
    top: "bn_conv_transition_07"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_transition_07"
    type: "ReLU"
    bottom: "bn_conv_transition_07"
    top: "bn_conv_transition_07"
}
layer {
    name: "conv_transition_07"
    type: "Convolution"
    bottom: "bn_conv_transition_07"
    top: "conv_transition_07"
    param {
        lr_mult: 1.000000
    }
    param {
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 1
        stride: 1
        pad: 0
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv08_01_a"
    type: "BatchNorm"
    bottom: "conv_transition_07"
    top: "bn_conv08_01_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv08_01_a"
    type: "Scale"
    bottom: "bn_conv08_01_a"
    top: "bn_conv08_01_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu08_01_a"
    type: "ReLU"
    bottom: "bn_conv08_01_a"
    top: "bn_conv08_01_a"
}
layer {
    name: "conv08_01_a"
    type: "Convolution"
    bottom: "bn_conv08_01_a"
    top: "conv08_01_a"
    param {
        name: "conv08_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv08_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv08_01_b"
    type: "BatchNorm"
    bottom: "conv08_01_a"
    top: "bn_conv08_01_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv08_01_b"
    type: "Scale"
    bottom: "bn_conv08_01_b"
    top: "bn_conv08_01_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu08_01_b"
    type: "ReLU"
    bottom: "bn_conv08_01_b"
    top: "bn_conv08_01_b"
}
layer {
    name: "conv08_01_b"
    type: "Convolution"
    bottom: "bn_conv08_01_b"
    top: "conv08_01_b"
    param {
        name: "conv08_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv08_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise08_01"
    type: "Eltwise"
    bottom: "conv_transition_07"
    bottom: "conv08_01_b"
    top: "eltwise08_01"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv08_02_a"
    type: "BatchNorm"
    bottom: "eltwise08_01"
    top: "bn_conv08_02_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv08_02_a"
    type: "Scale"
    bottom: "bn_conv08_02_a"
    top: "bn_conv08_02_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu08_02_a"
    type: "ReLU"
    bottom: "bn_conv08_02_a"
    top: "bn_conv08_02_a"
}
layer {
    name: "conv08_02_a"
    type: "Convolution"
    bottom: "bn_conv08_02_a"
    top: "conv08_02_a"
    param {
        name: "conv08_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv08_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv08_02_b"
    type: "BatchNorm"
    bottom: "conv08_02_a"
    top: "bn_conv08_02_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv08_02_b"
    type: "Scale"
    bottom: "bn_conv08_02_b"
    top: "bn_conv08_02_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu08_02_b"
    type: "ReLU"
    bottom: "bn_conv08_02_b"
    top: "bn_conv08_02_b"
}
layer {
    name: "conv08_02_b"
    type: "Convolution"
    bottom: "bn_conv08_02_b"
    top: "conv08_02_b"
    param {
        name: "conv08_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv08_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise08_02"
    type: "Eltwise"
    bottom: "eltwise08_01"
    bottom: "conv08_02_b"
    top: "eltwise08_02"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv08_03_a"
    type: "BatchNorm"
    bottom: "eltwise08_02"
    top: "bn_conv08_03_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv08_03_a"
    type: "Scale"
    bottom: "bn_conv08_03_a"
    top: "bn_conv08_03_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu08_03_a"
    type: "ReLU"
    bottom: "bn_conv08_03_a"
    top: "bn_conv08_03_a"
}
layer {
    name: "conv08_03_a"
    type: "Convolution"
    bottom: "bn_conv08_03_a"
    top: "conv08_03_a"
    param {
        name: "conv08_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv08_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv08_03_b"
    type: "BatchNorm"
    bottom: "conv08_03_a"
    top: "bn_conv08_03_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv08_03_b"
    type: "Scale"
    bottom: "bn_conv08_03_b"
    top: "bn_conv08_03_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu08_03_b"
    type: "ReLU"
    bottom: "bn_conv08_03_b"
    top: "bn_conv08_03_b"
}
layer {
    name: "conv08_03_b"
    type: "Convolution"
    bottom: "bn_conv08_03_b"
    top: "conv08_03_b"
    param {
        name: "conv08_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv08_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise08_03"
    type: "Eltwise"
    bottom: "eltwise08_02"
    bottom: "conv08_03_b"
    top: "eltwise08_03"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv08_04_a"
    type: "BatchNorm"
    bottom: "eltwise08_03"
    top: "bn_conv08_04_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv08_04_a"
    type: "Scale"
    bottom: "bn_conv08_04_a"
    top: "bn_conv08_04_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu08_04_a"
    type: "ReLU"
    bottom: "bn_conv08_04_a"
    top: "bn_conv08_04_a"
}
layer {
    name: "conv08_04_a"
    type: "Convolution"
    bottom: "bn_conv08_04_a"
    top: "conv08_04_a"
    param {
        name: "conv08_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv08_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv08_04_b"
    type: "BatchNorm"
    bottom: "conv08_04_a"
    top: "bn_conv08_04_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv08_04_b"
    type: "Scale"
    bottom: "bn_conv08_04_b"
    top: "bn_conv08_04_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu08_04_b"
    type: "ReLU"
    bottom: "bn_conv08_04_b"
    top: "bn_conv08_04_b"
}
layer {
    name: "conv08_04_b"
    type: "Convolution"
    bottom: "bn_conv08_04_b"
    top: "conv08_04_b"
    param {
        name: "conv08_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv08_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise08_04"
    type: "Eltwise"
    bottom: "eltwise08_03"
    bottom: "conv08_04_b"
    top: "eltwise08_04"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv08_05_a"
    type: "BatchNorm"
    bottom: "eltwise08_04"
    top: "bn_conv08_05_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv08_05_a"
    type: "Scale"
    bottom: "bn_conv08_05_a"
    top: "bn_conv08_05_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu08_05_a"
    type: "ReLU"
    bottom: "bn_conv08_05_a"
    top: "bn_conv08_05_a"
}
layer {
    name: "conv08_05_a"
    type: "Convolution"
    bottom: "bn_conv08_05_a"
    top: "conv08_05_a"
    param {
        name: "conv08_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv08_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv08_05_b"
    type: "BatchNorm"
    bottom: "conv08_05_a"
    top: "bn_conv08_05_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv08_05_b"
    type: "Scale"
    bottom: "bn_conv08_05_b"
    top: "bn_conv08_05_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu08_05_b"
    type: "ReLU"
    bottom: "bn_conv08_05_b"
    top: "bn_conv08_05_b"
}
layer {
    name: "conv08_05_b"
    type: "Convolution"
    bottom: "bn_conv08_05_b"
    top: "conv08_05_b"
    param {
        name: "conv08_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv08_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise08_05"
    type: "Eltwise"
    bottom: "eltwise08_04"
    bottom: "conv08_05_b"
    top: "eltwise08_05"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv08_06_a"
    type: "BatchNorm"
    bottom: "eltwise08_05"
    top: "bn_conv08_06_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv08_06_a"
    type: "Scale"
    bottom: "bn_conv08_06_a"
    top: "bn_conv08_06_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu08_06_a"
    type: "ReLU"
    bottom: "bn_conv08_06_a"
    top: "bn_conv08_06_a"
}
layer {
    name: "conv08_06_a"
    type: "Convolution"
    bottom: "bn_conv08_06_a"
    top: "conv08_06_a"
    param {
        name: "conv08_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv08_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv08_06_b"
    type: "BatchNorm"
    bottom: "conv08_06_a"
    top: "bn_conv08_06_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv08_06_b"
    type: "Scale"
    bottom: "bn_conv08_06_b"
    top: "bn_conv08_06_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu08_06_b"
    type: "ReLU"
    bottom: "bn_conv08_06_b"
    top: "bn_conv08_06_b"
}
layer {
    name: "conv08_06_b"
    type: "Convolution"
    bottom: "bn_conv08_06_b"
    top: "conv08_06_b"
    param {
        name: "conv08_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv08_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise08_06"
    type: "Eltwise"
    bottom: "eltwise08_05"
    bottom: "conv08_06_b"
    top: "eltwise08_06"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv08_07_a"
    type: "BatchNorm"
    bottom: "eltwise08_06"
    top: "bn_conv08_07_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv08_07_a"
    type: "Scale"
    bottom: "bn_conv08_07_a"
    top: "bn_conv08_07_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu08_07_a"
    type: "ReLU"
    bottom: "bn_conv08_07_a"
    top: "bn_conv08_07_a"
}
layer {
    name: "conv08_07_a"
    type: "Convolution"
    bottom: "bn_conv08_07_a"
    top: "conv08_07_a"
    param {
        name: "conv08_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv08_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv08_07_b"
    type: "BatchNorm"
    bottom: "conv08_07_a"
    top: "bn_conv08_07_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv08_07_b"
    type: "Scale"
    bottom: "bn_conv08_07_b"
    top: "bn_conv08_07_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu08_07_b"
    type: "ReLU"
    bottom: "bn_conv08_07_b"
    top: "bn_conv08_07_b"
}
layer {
    name: "conv08_07_b"
    type: "Convolution"
    bottom: "bn_conv08_07_b"
    top: "conv08_07_b"
    param {
        name: "conv08_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv08_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise08_07"
    type: "Eltwise"
    bottom: "eltwise08_06"
    bottom: "conv08_07_b"
    top: "eltwise08_07"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv08_08_a"
    type: "BatchNorm"
    bottom: "eltwise08_07"
    top: "bn_conv08_08_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv08_08_a"
    type: "Scale"
    bottom: "bn_conv08_08_a"
    top: "bn_conv08_08_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu08_08_a"
    type: "ReLU"
    bottom: "bn_conv08_08_a"
    top: "bn_conv08_08_a"
}
layer {
    name: "conv08_08_a"
    type: "Convolution"
    bottom: "bn_conv08_08_a"
    top: "conv08_08_a"
    param {
        name: "conv08_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv08_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv08_08_b"
    type: "BatchNorm"
    bottom: "conv08_08_a"
    top: "bn_conv08_08_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv08_08_b"
    type: "Scale"
    bottom: "bn_conv08_08_b"
    top: "bn_conv08_08_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu08_08_b"
    type: "ReLU"
    bottom: "bn_conv08_08_b"
    top: "bn_conv08_08_b"
}
layer {
    name: "conv08_08_b"
    type: "Convolution"
    bottom: "bn_conv08_08_b"
    top: "conv08_08_b"
    param {
        name: "conv08_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv08_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise08_08"
    type: "Eltwise"
    bottom: "eltwise08_07"
    bottom: "conv08_08_b"
    top: "eltwise08_08"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv08_09_a"
    type: "BatchNorm"
    bottom: "eltwise08_08"
    top: "bn_conv08_09_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv08_09_a"
    type: "Scale"
    bottom: "bn_conv08_09_a"
    top: "bn_conv08_09_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu08_09_a"
    type: "ReLU"
    bottom: "bn_conv08_09_a"
    top: "bn_conv08_09_a"
}
layer {
    name: "conv08_09_a"
    type: "Convolution"
    bottom: "bn_conv08_09_a"
    top: "conv08_09_a"
    param {
        name: "conv08_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv08_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv08_09_b"
    type: "BatchNorm"
    bottom: "conv08_09_a"
    top: "bn_conv08_09_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv08_09_b"
    type: "Scale"
    bottom: "bn_conv08_09_b"
    top: "bn_conv08_09_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu08_09_b"
    type: "ReLU"
    bottom: "bn_conv08_09_b"
    top: "bn_conv08_09_b"
}
layer {
    name: "conv08_09_b"
    type: "Convolution"
    bottom: "bn_conv08_09_b"
    top: "conv08_09_b"
    param {
        name: "conv08_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv08_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise08_09"
    type: "Eltwise"
    bottom: "eltwise08_08"
    bottom: "conv08_09_b"
    top: "eltwise08_09"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv08_10_a"
    type: "BatchNorm"
    bottom: "eltwise08_09"
    top: "bn_conv08_10_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv08_10_a"
    type: "Scale"
    bottom: "bn_conv08_10_a"
    top: "bn_conv08_10_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu08_10_a"
    type: "ReLU"
    bottom: "bn_conv08_10_a"
    top: "bn_conv08_10_a"
}
layer {
    name: "conv08_10_a"
    type: "Convolution"
    bottom: "bn_conv08_10_a"
    top: "conv08_10_a"
    param {
        name: "conv08_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv08_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv08_10_b"
    type: "BatchNorm"
    bottom: "conv08_10_a"
    top: "bn_conv08_10_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv08_10_b"
    type: "Scale"
    bottom: "bn_conv08_10_b"
    top: "bn_conv08_10_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu08_10_b"
    type: "ReLU"
    bottom: "bn_conv08_10_b"
    top: "bn_conv08_10_b"
}
layer {
    name: "conv08_10_b"
    type: "Convolution"
    bottom: "bn_conv08_10_b"
    top: "conv08_10_b"
    param {
        name: "conv08_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv08_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise08_10"
    type: "Eltwise"
    bottom: "eltwise08_09"
    bottom: "conv08_10_b"
    top: "eltwise08_10"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "concat08"
    type: "Concat"
    bottom: "conv1"
    bottom: "conv_transition_01"
    bottom: "conv_transition_02"
    bottom: "conv_transition_03"
    bottom: "conv_transition_04"
    bottom: "conv_transition_05"
    bottom: "conv_transition_06"
    bottom: "conv_transition_07"
    bottom: "eltwise08_01"
    bottom: "eltwise08_02"
    bottom: "eltwise08_03"
    bottom: "eltwise08_04"
    bottom: "eltwise08_05"
    bottom: "eltwise08_06"
    bottom: "eltwise08_07"
    bottom: "eltwise08_08"
    bottom: "eltwise08_09"
    bottom: "eltwise08_10"
    top: "concat08"
    concat_param {
        axis: 1
    }
}
layer {
    name: "bn_conv_transition_08"
    type: "BatchNorm"
    bottom: "concat08"
    top: "bn_conv_transition_08"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv_transition_08"
    type: "Scale"
    bottom: "bn_conv_transition_08"
    top: "bn_conv_transition_08"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_transition_08"
    type: "ReLU"
    bottom: "bn_conv_transition_08"
    top: "bn_conv_transition_08"
}
layer {
    name: "conv_transition_08"
    type: "Convolution"
    bottom: "bn_conv_transition_08"
    top: "conv_transition_08"
    param {
        lr_mult: 1.000000
    }
    param {
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 1
        stride: 1
        pad: 0
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv09_01_a"
    type: "BatchNorm"
    bottom: "conv_transition_08"
    top: "bn_conv09_01_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv09_01_a"
    type: "Scale"
    bottom: "bn_conv09_01_a"
    top: "bn_conv09_01_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu09_01_a"
    type: "ReLU"
    bottom: "bn_conv09_01_a"
    top: "bn_conv09_01_a"
}
layer {
    name: "conv09_01_a"
    type: "Convolution"
    bottom: "bn_conv09_01_a"
    top: "conv09_01_a"
    param {
        name: "conv09_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv09_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv09_01_b"
    type: "BatchNorm"
    bottom: "conv09_01_a"
    top: "bn_conv09_01_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv09_01_b"
    type: "Scale"
    bottom: "bn_conv09_01_b"
    top: "bn_conv09_01_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu09_01_b"
    type: "ReLU"
    bottom: "bn_conv09_01_b"
    top: "bn_conv09_01_b"
}
layer {
    name: "conv09_01_b"
    type: "Convolution"
    bottom: "bn_conv09_01_b"
    top: "conv09_01_b"
    param {
        name: "conv09_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv09_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise09_01"
    type: "Eltwise"
    bottom: "conv_transition_08"
    bottom: "conv09_01_b"
    top: "eltwise09_01"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv09_02_a"
    type: "BatchNorm"
    bottom: "eltwise09_01"
    top: "bn_conv09_02_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv09_02_a"
    type: "Scale"
    bottom: "bn_conv09_02_a"
    top: "bn_conv09_02_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu09_02_a"
    type: "ReLU"
    bottom: "bn_conv09_02_a"
    top: "bn_conv09_02_a"
}
layer {
    name: "conv09_02_a"
    type: "Convolution"
    bottom: "bn_conv09_02_a"
    top: "conv09_02_a"
    param {
        name: "conv09_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv09_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv09_02_b"
    type: "BatchNorm"
    bottom: "conv09_02_a"
    top: "bn_conv09_02_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv09_02_b"
    type: "Scale"
    bottom: "bn_conv09_02_b"
    top: "bn_conv09_02_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu09_02_b"
    type: "ReLU"
    bottom: "bn_conv09_02_b"
    top: "bn_conv09_02_b"
}
layer {
    name: "conv09_02_b"
    type: "Convolution"
    bottom: "bn_conv09_02_b"
    top: "conv09_02_b"
    param {
        name: "conv09_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv09_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise09_02"
    type: "Eltwise"
    bottom: "eltwise09_01"
    bottom: "conv09_02_b"
    top: "eltwise09_02"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv09_03_a"
    type: "BatchNorm"
    bottom: "eltwise09_02"
    top: "bn_conv09_03_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv09_03_a"
    type: "Scale"
    bottom: "bn_conv09_03_a"
    top: "bn_conv09_03_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu09_03_a"
    type: "ReLU"
    bottom: "bn_conv09_03_a"
    top: "bn_conv09_03_a"
}
layer {
    name: "conv09_03_a"
    type: "Convolution"
    bottom: "bn_conv09_03_a"
    top: "conv09_03_a"
    param {
        name: "conv09_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv09_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv09_03_b"
    type: "BatchNorm"
    bottom: "conv09_03_a"
    top: "bn_conv09_03_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv09_03_b"
    type: "Scale"
    bottom: "bn_conv09_03_b"
    top: "bn_conv09_03_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu09_03_b"
    type: "ReLU"
    bottom: "bn_conv09_03_b"
    top: "bn_conv09_03_b"
}
layer {
    name: "conv09_03_b"
    type: "Convolution"
    bottom: "bn_conv09_03_b"
    top: "conv09_03_b"
    param {
        name: "conv09_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv09_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise09_03"
    type: "Eltwise"
    bottom: "eltwise09_02"
    bottom: "conv09_03_b"
    top: "eltwise09_03"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv09_04_a"
    type: "BatchNorm"
    bottom: "eltwise09_03"
    top: "bn_conv09_04_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv09_04_a"
    type: "Scale"
    bottom: "bn_conv09_04_a"
    top: "bn_conv09_04_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu09_04_a"
    type: "ReLU"
    bottom: "bn_conv09_04_a"
    top: "bn_conv09_04_a"
}
layer {
    name: "conv09_04_a"
    type: "Convolution"
    bottom: "bn_conv09_04_a"
    top: "conv09_04_a"
    param {
        name: "conv09_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv09_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv09_04_b"
    type: "BatchNorm"
    bottom: "conv09_04_a"
    top: "bn_conv09_04_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv09_04_b"
    type: "Scale"
    bottom: "bn_conv09_04_b"
    top: "bn_conv09_04_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu09_04_b"
    type: "ReLU"
    bottom: "bn_conv09_04_b"
    top: "bn_conv09_04_b"
}
layer {
    name: "conv09_04_b"
    type: "Convolution"
    bottom: "bn_conv09_04_b"
    top: "conv09_04_b"
    param {
        name: "conv09_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv09_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise09_04"
    type: "Eltwise"
    bottom: "eltwise09_03"
    bottom: "conv09_04_b"
    top: "eltwise09_04"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv09_05_a"
    type: "BatchNorm"
    bottom: "eltwise09_04"
    top: "bn_conv09_05_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv09_05_a"
    type: "Scale"
    bottom: "bn_conv09_05_a"
    top: "bn_conv09_05_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu09_05_a"
    type: "ReLU"
    bottom: "bn_conv09_05_a"
    top: "bn_conv09_05_a"
}
layer {
    name: "conv09_05_a"
    type: "Convolution"
    bottom: "bn_conv09_05_a"
    top: "conv09_05_a"
    param {
        name: "conv09_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv09_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv09_05_b"
    type: "BatchNorm"
    bottom: "conv09_05_a"
    top: "bn_conv09_05_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv09_05_b"
    type: "Scale"
    bottom: "bn_conv09_05_b"
    top: "bn_conv09_05_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu09_05_b"
    type: "ReLU"
    bottom: "bn_conv09_05_b"
    top: "bn_conv09_05_b"
}
layer {
    name: "conv09_05_b"
    type: "Convolution"
    bottom: "bn_conv09_05_b"
    top: "conv09_05_b"
    param {
        name: "conv09_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv09_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise09_05"
    type: "Eltwise"
    bottom: "eltwise09_04"
    bottom: "conv09_05_b"
    top: "eltwise09_05"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv09_06_a"
    type: "BatchNorm"
    bottom: "eltwise09_05"
    top: "bn_conv09_06_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv09_06_a"
    type: "Scale"
    bottom: "bn_conv09_06_a"
    top: "bn_conv09_06_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu09_06_a"
    type: "ReLU"
    bottom: "bn_conv09_06_a"
    top: "bn_conv09_06_a"
}
layer {
    name: "conv09_06_a"
    type: "Convolution"
    bottom: "bn_conv09_06_a"
    top: "conv09_06_a"
    param {
        name: "conv09_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv09_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv09_06_b"
    type: "BatchNorm"
    bottom: "conv09_06_a"
    top: "bn_conv09_06_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv09_06_b"
    type: "Scale"
    bottom: "bn_conv09_06_b"
    top: "bn_conv09_06_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu09_06_b"
    type: "ReLU"
    bottom: "bn_conv09_06_b"
    top: "bn_conv09_06_b"
}
layer {
    name: "conv09_06_b"
    type: "Convolution"
    bottom: "bn_conv09_06_b"
    top: "conv09_06_b"
    param {
        name: "conv09_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv09_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise09_06"
    type: "Eltwise"
    bottom: "eltwise09_05"
    bottom: "conv09_06_b"
    top: "eltwise09_06"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv09_07_a"
    type: "BatchNorm"
    bottom: "eltwise09_06"
    top: "bn_conv09_07_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv09_07_a"
    type: "Scale"
    bottom: "bn_conv09_07_a"
    top: "bn_conv09_07_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu09_07_a"
    type: "ReLU"
    bottom: "bn_conv09_07_a"
    top: "bn_conv09_07_a"
}
layer {
    name: "conv09_07_a"
    type: "Convolution"
    bottom: "bn_conv09_07_a"
    top: "conv09_07_a"
    param {
        name: "conv09_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv09_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv09_07_b"
    type: "BatchNorm"
    bottom: "conv09_07_a"
    top: "bn_conv09_07_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv09_07_b"
    type: "Scale"
    bottom: "bn_conv09_07_b"
    top: "bn_conv09_07_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu09_07_b"
    type: "ReLU"
    bottom: "bn_conv09_07_b"
    top: "bn_conv09_07_b"
}
layer {
    name: "conv09_07_b"
    type: "Convolution"
    bottom: "bn_conv09_07_b"
    top: "conv09_07_b"
    param {
        name: "conv09_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv09_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise09_07"
    type: "Eltwise"
    bottom: "eltwise09_06"
    bottom: "conv09_07_b"
    top: "eltwise09_07"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv09_08_a"
    type: "BatchNorm"
    bottom: "eltwise09_07"
    top: "bn_conv09_08_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv09_08_a"
    type: "Scale"
    bottom: "bn_conv09_08_a"
    top: "bn_conv09_08_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu09_08_a"
    type: "ReLU"
    bottom: "bn_conv09_08_a"
    top: "bn_conv09_08_a"
}
layer {
    name: "conv09_08_a"
    type: "Convolution"
    bottom: "bn_conv09_08_a"
    top: "conv09_08_a"
    param {
        name: "conv09_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv09_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv09_08_b"
    type: "BatchNorm"
    bottom: "conv09_08_a"
    top: "bn_conv09_08_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv09_08_b"
    type: "Scale"
    bottom: "bn_conv09_08_b"
    top: "bn_conv09_08_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu09_08_b"
    type: "ReLU"
    bottom: "bn_conv09_08_b"
    top: "bn_conv09_08_b"
}
layer {
    name: "conv09_08_b"
    type: "Convolution"
    bottom: "bn_conv09_08_b"
    top: "conv09_08_b"
    param {
        name: "conv09_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv09_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise09_08"
    type: "Eltwise"
    bottom: "eltwise09_07"
    bottom: "conv09_08_b"
    top: "eltwise09_08"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv09_09_a"
    type: "BatchNorm"
    bottom: "eltwise09_08"
    top: "bn_conv09_09_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv09_09_a"
    type: "Scale"
    bottom: "bn_conv09_09_a"
    top: "bn_conv09_09_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu09_09_a"
    type: "ReLU"
    bottom: "bn_conv09_09_a"
    top: "bn_conv09_09_a"
}
layer {
    name: "conv09_09_a"
    type: "Convolution"
    bottom: "bn_conv09_09_a"
    top: "conv09_09_a"
    param {
        name: "conv09_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv09_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv09_09_b"
    type: "BatchNorm"
    bottom: "conv09_09_a"
    top: "bn_conv09_09_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv09_09_b"
    type: "Scale"
    bottom: "bn_conv09_09_b"
    top: "bn_conv09_09_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu09_09_b"
    type: "ReLU"
    bottom: "bn_conv09_09_b"
    top: "bn_conv09_09_b"
}
layer {
    name: "conv09_09_b"
    type: "Convolution"
    bottom: "bn_conv09_09_b"
    top: "conv09_09_b"
    param {
        name: "conv09_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv09_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise09_09"
    type: "Eltwise"
    bottom: "eltwise09_08"
    bottom: "conv09_09_b"
    top: "eltwise09_09"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv09_10_a"
    type: "BatchNorm"
    bottom: "eltwise09_09"
    top: "bn_conv09_10_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv09_10_a"
    type: "Scale"
    bottom: "bn_conv09_10_a"
    top: "bn_conv09_10_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu09_10_a"
    type: "ReLU"
    bottom: "bn_conv09_10_a"
    top: "bn_conv09_10_a"
}
layer {
    name: "conv09_10_a"
    type: "Convolution"
    bottom: "bn_conv09_10_a"
    top: "conv09_10_a"
    param {
        name: "conv09_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv09_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv09_10_b"
    type: "BatchNorm"
    bottom: "conv09_10_a"
    top: "bn_conv09_10_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv09_10_b"
    type: "Scale"
    bottom: "bn_conv09_10_b"
    top: "bn_conv09_10_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu09_10_b"
    type: "ReLU"
    bottom: "bn_conv09_10_b"
    top: "bn_conv09_10_b"
}
layer {
    name: "conv09_10_b"
    type: "Convolution"
    bottom: "bn_conv09_10_b"
    top: "conv09_10_b"
    param {
        name: "conv09_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv09_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise09_10"
    type: "Eltwise"
    bottom: "eltwise09_09"
    bottom: "conv09_10_b"
    top: "eltwise09_10"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "concat09"
    type: "Concat"
    bottom: "conv1"
    bottom: "conv_transition_01"
    bottom: "conv_transition_02"
    bottom: "conv_transition_03"
    bottom: "conv_transition_04"
    bottom: "conv_transition_05"
    bottom: "conv_transition_06"
    bottom: "conv_transition_07"
    bottom: "conv_transition_08"
    bottom: "eltwise09_01"
    bottom: "eltwise09_02"
    bottom: "eltwise09_03"
    bottom: "eltwise09_04"
    bottom: "eltwise09_05"
    bottom: "eltwise09_06"
    bottom: "eltwise09_07"
    bottom: "eltwise09_08"
    bottom: "eltwise09_09"
    bottom: "eltwise09_10"
    top: "concat09"
    concat_param {
        axis: 1
    }
}
layer {
    name: "bn_conv_transition_09"
    type: "BatchNorm"
    bottom: "concat09"
    top: "bn_conv_transition_09"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv_transition_09"
    type: "Scale"
    bottom: "bn_conv_transition_09"
    top: "bn_conv_transition_09"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_transition_09"
    type: "ReLU"
    bottom: "bn_conv_transition_09"
    top: "bn_conv_transition_09"
}
layer {
    name: "conv_transition_09"
    type: "Convolution"
    bottom: "bn_conv_transition_09"
    top: "conv_transition_09"
    param {
        lr_mult: 1.000000
    }
    param {
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 1
        stride: 1
        pad: 0
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv10_01_a"
    type: "BatchNorm"
    bottom: "conv_transition_09"
    top: "bn_conv10_01_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv10_01_a"
    type: "Scale"
    bottom: "bn_conv10_01_a"
    top: "bn_conv10_01_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu10_01_a"
    type: "ReLU"
    bottom: "bn_conv10_01_a"
    top: "bn_conv10_01_a"
}
layer {
    name: "conv10_01_a"
    type: "Convolution"
    bottom: "bn_conv10_01_a"
    top: "conv10_01_a"
    param {
        name: "conv10_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv10_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv10_01_b"
    type: "BatchNorm"
    bottom: "conv10_01_a"
    top: "bn_conv10_01_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv10_01_b"
    type: "Scale"
    bottom: "bn_conv10_01_b"
    top: "bn_conv10_01_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu10_01_b"
    type: "ReLU"
    bottom: "bn_conv10_01_b"
    top: "bn_conv10_01_b"
}
layer {
    name: "conv10_01_b"
    type: "Convolution"
    bottom: "bn_conv10_01_b"
    top: "conv10_01_b"
    param {
        name: "conv10_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv10_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise10_01"
    type: "Eltwise"
    bottom: "conv_transition_09"
    bottom: "conv10_01_b"
    top: "eltwise10_01"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv10_02_a"
    type: "BatchNorm"
    bottom: "eltwise10_01"
    top: "bn_conv10_02_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv10_02_a"
    type: "Scale"
    bottom: "bn_conv10_02_a"
    top: "bn_conv10_02_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu10_02_a"
    type: "ReLU"
    bottom: "bn_conv10_02_a"
    top: "bn_conv10_02_a"
}
layer {
    name: "conv10_02_a"
    type: "Convolution"
    bottom: "bn_conv10_02_a"
    top: "conv10_02_a"
    param {
        name: "conv10_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv10_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv10_02_b"
    type: "BatchNorm"
    bottom: "conv10_02_a"
    top: "bn_conv10_02_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv10_02_b"
    type: "Scale"
    bottom: "bn_conv10_02_b"
    top: "bn_conv10_02_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu10_02_b"
    type: "ReLU"
    bottom: "bn_conv10_02_b"
    top: "bn_conv10_02_b"
}
layer {
    name: "conv10_02_b"
    type: "Convolution"
    bottom: "bn_conv10_02_b"
    top: "conv10_02_b"
    param {
        name: "conv10_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv10_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise10_02"
    type: "Eltwise"
    bottom: "eltwise10_01"
    bottom: "conv10_02_b"
    top: "eltwise10_02"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv10_03_a"
    type: "BatchNorm"
    bottom: "eltwise10_02"
    top: "bn_conv10_03_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv10_03_a"
    type: "Scale"
    bottom: "bn_conv10_03_a"
    top: "bn_conv10_03_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu10_03_a"
    type: "ReLU"
    bottom: "bn_conv10_03_a"
    top: "bn_conv10_03_a"
}
layer {
    name: "conv10_03_a"
    type: "Convolution"
    bottom: "bn_conv10_03_a"
    top: "conv10_03_a"
    param {
        name: "conv10_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv10_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv10_03_b"
    type: "BatchNorm"
    bottom: "conv10_03_a"
    top: "bn_conv10_03_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv10_03_b"
    type: "Scale"
    bottom: "bn_conv10_03_b"
    top: "bn_conv10_03_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu10_03_b"
    type: "ReLU"
    bottom: "bn_conv10_03_b"
    top: "bn_conv10_03_b"
}
layer {
    name: "conv10_03_b"
    type: "Convolution"
    bottom: "bn_conv10_03_b"
    top: "conv10_03_b"
    param {
        name: "conv10_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv10_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise10_03"
    type: "Eltwise"
    bottom: "eltwise10_02"
    bottom: "conv10_03_b"
    top: "eltwise10_03"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv10_04_a"
    type: "BatchNorm"
    bottom: "eltwise10_03"
    top: "bn_conv10_04_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv10_04_a"
    type: "Scale"
    bottom: "bn_conv10_04_a"
    top: "bn_conv10_04_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu10_04_a"
    type: "ReLU"
    bottom: "bn_conv10_04_a"
    top: "bn_conv10_04_a"
}
layer {
    name: "conv10_04_a"
    type: "Convolution"
    bottom: "bn_conv10_04_a"
    top: "conv10_04_a"
    param {
        name: "conv10_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv10_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv10_04_b"
    type: "BatchNorm"
    bottom: "conv10_04_a"
    top: "bn_conv10_04_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv10_04_b"
    type: "Scale"
    bottom: "bn_conv10_04_b"
    top: "bn_conv10_04_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu10_04_b"
    type: "ReLU"
    bottom: "bn_conv10_04_b"
    top: "bn_conv10_04_b"
}
layer {
    name: "conv10_04_b"
    type: "Convolution"
    bottom: "bn_conv10_04_b"
    top: "conv10_04_b"
    param {
        name: "conv10_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv10_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise10_04"
    type: "Eltwise"
    bottom: "eltwise10_03"
    bottom: "conv10_04_b"
    top: "eltwise10_04"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv10_05_a"
    type: "BatchNorm"
    bottom: "eltwise10_04"
    top: "bn_conv10_05_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv10_05_a"
    type: "Scale"
    bottom: "bn_conv10_05_a"
    top: "bn_conv10_05_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu10_05_a"
    type: "ReLU"
    bottom: "bn_conv10_05_a"
    top: "bn_conv10_05_a"
}
layer {
    name: "conv10_05_a"
    type: "Convolution"
    bottom: "bn_conv10_05_a"
    top: "conv10_05_a"
    param {
        name: "conv10_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv10_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv10_05_b"
    type: "BatchNorm"
    bottom: "conv10_05_a"
    top: "bn_conv10_05_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv10_05_b"
    type: "Scale"
    bottom: "bn_conv10_05_b"
    top: "bn_conv10_05_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu10_05_b"
    type: "ReLU"
    bottom: "bn_conv10_05_b"
    top: "bn_conv10_05_b"
}
layer {
    name: "conv10_05_b"
    type: "Convolution"
    bottom: "bn_conv10_05_b"
    top: "conv10_05_b"
    param {
        name: "conv10_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv10_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise10_05"
    type: "Eltwise"
    bottom: "eltwise10_04"
    bottom: "conv10_05_b"
    top: "eltwise10_05"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv10_06_a"
    type: "BatchNorm"
    bottom: "eltwise10_05"
    top: "bn_conv10_06_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv10_06_a"
    type: "Scale"
    bottom: "bn_conv10_06_a"
    top: "bn_conv10_06_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu10_06_a"
    type: "ReLU"
    bottom: "bn_conv10_06_a"
    top: "bn_conv10_06_a"
}
layer {
    name: "conv10_06_a"
    type: "Convolution"
    bottom: "bn_conv10_06_a"
    top: "conv10_06_a"
    param {
        name: "conv10_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv10_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv10_06_b"
    type: "BatchNorm"
    bottom: "conv10_06_a"
    top: "bn_conv10_06_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv10_06_b"
    type: "Scale"
    bottom: "bn_conv10_06_b"
    top: "bn_conv10_06_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu10_06_b"
    type: "ReLU"
    bottom: "bn_conv10_06_b"
    top: "bn_conv10_06_b"
}
layer {
    name: "conv10_06_b"
    type: "Convolution"
    bottom: "bn_conv10_06_b"
    top: "conv10_06_b"
    param {
        name: "conv10_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv10_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise10_06"
    type: "Eltwise"
    bottom: "eltwise10_05"
    bottom: "conv10_06_b"
    top: "eltwise10_06"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv10_07_a"
    type: "BatchNorm"
    bottom: "eltwise10_06"
    top: "bn_conv10_07_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv10_07_a"
    type: "Scale"
    bottom: "bn_conv10_07_a"
    top: "bn_conv10_07_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu10_07_a"
    type: "ReLU"
    bottom: "bn_conv10_07_a"
    top: "bn_conv10_07_a"
}
layer {
    name: "conv10_07_a"
    type: "Convolution"
    bottom: "bn_conv10_07_a"
    top: "conv10_07_a"
    param {
        name: "conv10_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv10_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv10_07_b"
    type: "BatchNorm"
    bottom: "conv10_07_a"
    top: "bn_conv10_07_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv10_07_b"
    type: "Scale"
    bottom: "bn_conv10_07_b"
    top: "bn_conv10_07_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu10_07_b"
    type: "ReLU"
    bottom: "bn_conv10_07_b"
    top: "bn_conv10_07_b"
}
layer {
    name: "conv10_07_b"
    type: "Convolution"
    bottom: "bn_conv10_07_b"
    top: "conv10_07_b"
    param {
        name: "conv10_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv10_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise10_07"
    type: "Eltwise"
    bottom: "eltwise10_06"
    bottom: "conv10_07_b"
    top: "eltwise10_07"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv10_08_a"
    type: "BatchNorm"
    bottom: "eltwise10_07"
    top: "bn_conv10_08_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv10_08_a"
    type: "Scale"
    bottom: "bn_conv10_08_a"
    top: "bn_conv10_08_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu10_08_a"
    type: "ReLU"
    bottom: "bn_conv10_08_a"
    top: "bn_conv10_08_a"
}
layer {
    name: "conv10_08_a"
    type: "Convolution"
    bottom: "bn_conv10_08_a"
    top: "conv10_08_a"
    param {
        name: "conv10_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv10_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv10_08_b"
    type: "BatchNorm"
    bottom: "conv10_08_a"
    top: "bn_conv10_08_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv10_08_b"
    type: "Scale"
    bottom: "bn_conv10_08_b"
    top: "bn_conv10_08_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu10_08_b"
    type: "ReLU"
    bottom: "bn_conv10_08_b"
    top: "bn_conv10_08_b"
}
layer {
    name: "conv10_08_b"
    type: "Convolution"
    bottom: "bn_conv10_08_b"
    top: "conv10_08_b"
    param {
        name: "conv10_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv10_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise10_08"
    type: "Eltwise"
    bottom: "eltwise10_07"
    bottom: "conv10_08_b"
    top: "eltwise10_08"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv10_09_a"
    type: "BatchNorm"
    bottom: "eltwise10_08"
    top: "bn_conv10_09_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv10_09_a"
    type: "Scale"
    bottom: "bn_conv10_09_a"
    top: "bn_conv10_09_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu10_09_a"
    type: "ReLU"
    bottom: "bn_conv10_09_a"
    top: "bn_conv10_09_a"
}
layer {
    name: "conv10_09_a"
    type: "Convolution"
    bottom: "bn_conv10_09_a"
    top: "conv10_09_a"
    param {
        name: "conv10_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv10_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv10_09_b"
    type: "BatchNorm"
    bottom: "conv10_09_a"
    top: "bn_conv10_09_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv10_09_b"
    type: "Scale"
    bottom: "bn_conv10_09_b"
    top: "bn_conv10_09_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu10_09_b"
    type: "ReLU"
    bottom: "bn_conv10_09_b"
    top: "bn_conv10_09_b"
}
layer {
    name: "conv10_09_b"
    type: "Convolution"
    bottom: "bn_conv10_09_b"
    top: "conv10_09_b"
    param {
        name: "conv10_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv10_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise10_09"
    type: "Eltwise"
    bottom: "eltwise10_08"
    bottom: "conv10_09_b"
    top: "eltwise10_09"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "bn_conv10_10_a"
    type: "BatchNorm"
    bottom: "eltwise10_09"
    top: "bn_conv10_10_a"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv10_10_a"
    type: "Scale"
    bottom: "bn_conv10_10_a"
    top: "bn_conv10_10_a"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu10_10_a"
    type: "ReLU"
    bottom: "bn_conv10_10_a"
    top: "bn_conv10_10_a"
}
layer {
    name: "conv10_10_a"
    type: "Convolution"
    bottom: "bn_conv10_10_a"
    top: "conv10_10_a"
    param {
        name: "conv10_wa"
        lr_mult: 1.000000
    }
    param {
        name: "conv10_ba"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv10_10_b"
    type: "BatchNorm"
    bottom: "conv10_10_a"
    top: "bn_conv10_10_b"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv10_10_b"
    type: "Scale"
    bottom: "bn_conv10_10_b"
    top: "bn_conv10_10_b"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu10_10_b"
    type: "ReLU"
    bottom: "bn_conv10_10_b"
    top: "bn_conv10_10_b"
}
layer {
    name: "conv10_10_b"
    type: "Convolution"
    bottom: "bn_conv10_10_b"
    top: "conv10_10_b"
    param {
        name: "conv10_wb"
        lr_mult: 1.000000
    }
    param {
        name: "conv10_bb"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "eltwise10_10"
    type: "Eltwise"
    bottom: "eltwise10_09"
    bottom: "conv10_10_b"
    top: "eltwise10_10"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "concat10"
    type: "Concat"
    bottom: "conv1"
    bottom: "conv_transition_01"
    bottom: "conv_transition_02"
    bottom: "conv_transition_03"
    bottom: "conv_transition_04"
    bottom: "conv_transition_05"
    bottom: "conv_transition_06"
    bottom: "conv_transition_07"
    bottom: "conv_transition_08"
    bottom: "conv_transition_09"
    bottom: "eltwise10_01"
    bottom: "eltwise10_02"
    bottom: "eltwise10_03"
    bottom: "eltwise10_04"
    bottom: "eltwise10_05"
    bottom: "eltwise10_06"
    bottom: "eltwise10_07"
    bottom: "eltwise10_08"
    bottom: "eltwise10_09"
    bottom: "eltwise10_10"
    top: "concat10"
    concat_param {
        axis: 1
    }
}
layer {
    name: "bn_conv_transition_10"
    type: "BatchNorm"
    bottom: "concat10"
    top: "bn_conv_transition_10"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv_transition_10"
    type: "Scale"
    bottom: "bn_conv_transition_10"
    top: "bn_conv_transition_10"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_transition_10"
    type: "ReLU"
    bottom: "bn_conv_transition_10"
    top: "bn_conv_transition_10"
}
layer {
    name: "conv_transition_10"
    type: "Convolution"
    bottom: "bn_conv_transition_10"
    top: "conv_transition_10"
    param {
        lr_mult: 1.000000
    }
    param {
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 64
        kernel_size: 1
        stride: 1
        pad: 0
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "bn_conv_end_01"
    type: "BatchNorm"
    bottom: "conv_transition_01"
    top: "bn_conv_end_01"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv_end_01"
    type: "Scale"
    bottom: "bn_conv_end_01"
    top: "bn_conv_end_01"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_end_01"
    type: "ReLU"
    bottom: "bn_conv_end_01"
    top: "bn_conv_end_01"
}
layer {
    name: "conv_end_01"
    type: "Convolution"
    bottom: "bn_conv_end_01"
    top: "conv_end_01"
    param {
        name: "Recon_w"
        lr_mult: 1.000000
    }
    param {
        name: "Recon_b"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 1
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "HR_recovery_01"
    type: "Eltwise"
    bottom: "data"
    bottom: "conv_end_01"
    top: "HR_recovery_01"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "weight_output_end_01"
    type: "Scale"
    bottom: "HR_recovery_01"
    top: "weight_output_end_01"
    scale_param {
        bias_term: false
    }
}
layer {
    name: "bn_conv_end_02"
    type: "BatchNorm"
    bottom: "conv_transition_02"
    top: "bn_conv_end_02"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv_end_02"
    type: "Scale"
    bottom: "bn_conv_end_02"
    top: "bn_conv_end_02"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_end_02"
    type: "ReLU"
    bottom: "bn_conv_end_02"
    top: "bn_conv_end_02"
}
layer {
    name: "conv_end_02"
    type: "Convolution"
    bottom: "bn_conv_end_02"
    top: "conv_end_02"
    param {
        name: "Recon_w"
        lr_mult: 1.000000
    }
    param {
        name: "Recon_b"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 1
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "HR_recovery_02"
    type: "Eltwise"
    bottom: "data"
    bottom: "conv_end_02"
    top: "HR_recovery_02"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "weight_output_end_02"
    type: "Scale"
    bottom: "HR_recovery_02"
    top: "weight_output_end_02"
    scale_param {
        bias_term: false
    }
}
layer {
    name: "bn_conv_end_03"
    type: "BatchNorm"
    bottom: "conv_transition_03"
    top: "bn_conv_end_03"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv_end_03"
    type: "Scale"
    bottom: "bn_conv_end_03"
    top: "bn_conv_end_03"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_end_03"
    type: "ReLU"
    bottom: "bn_conv_end_03"
    top: "bn_conv_end_03"
}
layer {
    name: "conv_end_03"
    type: "Convolution"
    bottom: "bn_conv_end_03"
    top: "conv_end_03"
    param {
        name: "Recon_w"
        lr_mult: 1.000000
    }
    param {
        name: "Recon_b"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 1
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "HR_recovery_03"
    type: "Eltwise"
    bottom: "data"
    bottom: "conv_end_03"
    top: "HR_recovery_03"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "weight_output_end_03"
    type: "Scale"
    bottom: "HR_recovery_03"
    top: "weight_output_end_03"
    scale_param {
        bias_term: false
    }
}
layer {
    name: "bn_conv_end_04"
    type: "BatchNorm"
    bottom: "conv_transition_04"
    top: "bn_conv_end_04"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv_end_04"
    type: "Scale"
    bottom: "bn_conv_end_04"
    top: "bn_conv_end_04"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_end_04"
    type: "ReLU"
    bottom: "bn_conv_end_04"
    top: "bn_conv_end_04"
}
layer {
    name: "conv_end_04"
    type: "Convolution"
    bottom: "bn_conv_end_04"
    top: "conv_end_04"
    param {
        name: "Recon_w"
        lr_mult: 1.000000
    }
    param {
        name: "Recon_b"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 1
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "HR_recovery_04"
    type: "Eltwise"
    bottom: "data"
    bottom: "conv_end_04"
    top: "HR_recovery_04"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "weight_output_end_04"
    type: "Scale"
    bottom: "HR_recovery_04"
    top: "weight_output_end_04"
    scale_param {
        bias_term: false
    }
}
layer {
    name: "bn_conv_end_05"
    type: "BatchNorm"
    bottom: "conv_transition_05"
    top: "bn_conv_end_05"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv_end_05"
    type: "Scale"
    bottom: "bn_conv_end_05"
    top: "bn_conv_end_05"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_end_05"
    type: "ReLU"
    bottom: "bn_conv_end_05"
    top: "bn_conv_end_05"
}
layer {
    name: "conv_end_05"
    type: "Convolution"
    bottom: "bn_conv_end_05"
    top: "conv_end_05"
    param {
        name: "Recon_w"
        lr_mult: 1.000000
    }
    param {
        name: "Recon_b"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 1
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "HR_recovery_05"
    type: "Eltwise"
    bottom: "data"
    bottom: "conv_end_05"
    top: "HR_recovery_05"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "weight_output_end_05"
    type: "Scale"
    bottom: "HR_recovery_05"
    top: "weight_output_end_05"
    scale_param {
        bias_term: false
    }
}
layer {
    name: "bn_conv_end_06"
    type: "BatchNorm"
    bottom: "conv_transition_06"
    top: "bn_conv_end_06"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv_end_06"
    type: "Scale"
    bottom: "bn_conv_end_06"
    top: "bn_conv_end_06"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_end_06"
    type: "ReLU"
    bottom: "bn_conv_end_06"
    top: "bn_conv_end_06"
}
layer {
    name: "conv_end_06"
    type: "Convolution"
    bottom: "bn_conv_end_06"
    top: "conv_end_06"
    param {
        name: "Recon_w"
        lr_mult: 1.000000
    }
    param {
        name: "Recon_b"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 1
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "HR_recovery_06"
    type: "Eltwise"
    bottom: "data"
    bottom: "conv_end_06"
    top: "HR_recovery_06"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "weight_output_end_06"
    type: "Scale"
    bottom: "HR_recovery_06"
    top: "weight_output_end_06"
    scale_param {
        bias_term: false
    }
}
layer {
    name: "bn_conv_end_07"
    type: "BatchNorm"
    bottom: "conv_transition_07"
    top: "bn_conv_end_07"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv_end_07"
    type: "Scale"
    bottom: "bn_conv_end_07"
    top: "bn_conv_end_07"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_end_07"
    type: "ReLU"
    bottom: "bn_conv_end_07"
    top: "bn_conv_end_07"
}
layer {
    name: "conv_end_07"
    type: "Convolution"
    bottom: "bn_conv_end_07"
    top: "conv_end_07"
    param {
        name: "Recon_w"
        lr_mult: 1.000000
    }
    param {
        name: "Recon_b"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 1
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "HR_recovery_07"
    type: "Eltwise"
    bottom: "data"
    bottom: "conv_end_07"
    top: "HR_recovery_07"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "weight_output_end_07"
    type: "Scale"
    bottom: "HR_recovery_07"
    top: "weight_output_end_07"
    scale_param {
        bias_term: false
    }
}
layer {
    name: "bn_conv_end_08"
    type: "BatchNorm"
    bottom: "conv_transition_08"
    top: "bn_conv_end_08"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv_end_08"
    type: "Scale"
    bottom: "bn_conv_end_08"
    top: "bn_conv_end_08"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_end_08"
    type: "ReLU"
    bottom: "bn_conv_end_08"
    top: "bn_conv_end_08"
}
layer {
    name: "conv_end_08"
    type: "Convolution"
    bottom: "bn_conv_end_08"
    top: "conv_end_08"
    param {
        name: "Recon_w"
        lr_mult: 1.000000
    }
    param {
        name: "Recon_b"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 1
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "HR_recovery_08"
    type: "Eltwise"
    bottom: "data"
    bottom: "conv_end_08"
    top: "HR_recovery_08"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "weight_output_end_08"
    type: "Scale"
    bottom: "HR_recovery_08"
    top: "weight_output_end_08"
    scale_param {
        bias_term: false
    }
}
layer {
    name: "bn_conv_end_09"
    type: "BatchNorm"
    bottom: "conv_transition_09"
    top: "bn_conv_end_09"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv_end_09"
    type: "Scale"
    bottom: "bn_conv_end_09"
    top: "bn_conv_end_09"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_end_09"
    type: "ReLU"
    bottom: "bn_conv_end_09"
    top: "bn_conv_end_09"
}
layer {
    name: "conv_end_09"
    type: "Convolution"
    bottom: "bn_conv_end_09"
    top: "conv_end_09"
    param {
        name: "Recon_w"
        lr_mult: 1.000000
    }
    param {
        name: "Recon_b"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 1
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "HR_recovery_09"
    type: "Eltwise"
    bottom: "data"
    bottom: "conv_end_09"
    top: "HR_recovery_09"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "weight_output_end_09"
    type: "Scale"
    bottom: "HR_recovery_09"
    top: "weight_output_end_09"
    scale_param {
        bias_term: false
    }
}
layer {
    name: "bn_conv_end_10"
    type: "BatchNorm"
    bottom: "conv_transition_10"
    top: "bn_conv_end_10"
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
    param {
        lr_mult: 0
    }
}
layer {
    name: "scale_conv_end_10"
    type: "Scale"
    bottom: "bn_conv_end_10"
    top: "bn_conv_end_10"
    scale_param {
        bias_term: true
    }
}
layer {
    name: "relu_end_10"
    type: "ReLU"
    bottom: "bn_conv_end_10"
    top: "bn_conv_end_10"
}
layer {
    name: "conv_end_10"
    type: "Convolution"
    bottom: "bn_conv_end_10"
    top: "conv_end_10"
    param {
        name: "Recon_w"
        lr_mult: 1.000000
    }
    param {
        name: "Recon_b"
        lr_mult: 0.100000
    }
    convolution_param {
        num_output: 1
        kernel_size: 3
        stride: 1
        pad: 1
        weight_filler {
            type: "msra"
        }
        bias_filler {
            type: "constant"
            value: 0
        }
    }
}
layer {
    name: "HR_recovery_10"
    type: "Eltwise"
    bottom: "data"
    bottom: "conv_end_10"
    top: "HR_recovery_10"
    eltwise_param {
        operation: SUM
    }
}
layer {
    name: "weight_output_end_10"
    type: "Scale"
    bottom: "HR_recovery_10"
    top: "weight_output_end_10"
    scale_param {
        bias_term: false
    }
}
layer {
    name: "HR_recovery"
    type: "Eltwise"
    bottom: "weight_output_end_01"
    bottom: "weight_output_end_02"
    bottom: "weight_output_end_03"
    bottom: "weight_output_end_04"
    bottom: "weight_output_end_05"
    bottom: "weight_output_end_06"
    bottom: "weight_output_end_07"
    bottom: "weight_output_end_08"
    bottom: "weight_output_end_09"
    bottom: "weight_output_end_10"
    top: "HR_recovery"
    eltwise_param {
        operation: SUM
    }
}
